#!/usr/bin/env python
# -*- coding: utf-8 -*-

import logging
import os
import time
import json
import platform
import re
import hashlib
import datetime
import threading
from abc import ABC, abstractmethod
from dataclasses import dataclass, field
from typing import Dict, Any, List, Optional, Tuple
from urllib.parse import urljoin, urlparse

from src.crawlers.common.sync_decorator import sync_to_database_decorator
from src.crawlers.common.content_parser import ContentParser, DateExtractor, content_parser, date_extractor
from src.storage.file_storage import FileStorage, MarkdownGenerator

import requests
from bs4 import BeautifulSoup

# å°è¯•å¯¼å…¥html2text
try:
    import html2text
    HTML2TEXT_AVAILABLE = True
except ImportError:
    HTML2TEXT_AVAILABLE = False

logger = logging.getLogger(__name__)


@dataclass
class CrawlReport:
    """çˆ¬å–æŠ¥å‘Šæ•°æ®ç»“æ„"""
    vendor: str = ''
    source_type: str = ''
    total_discovered: int = 0         # æ€»å‘ç°æ•°
    new_saved: int = 0                # æ–°å¢ä¿å­˜æ•°
    skipped_exists: int = 0           # è·³è¿‡ï¼ˆå·²å­˜åœ¨ï¼‰
    skipped_ai_cleaned: int = 0       # è·³è¿‡ï¼ˆAIæ¸…æ´—è¿‡ï¼‰
    failed: int = 0                   # å¤±è´¥æ•°
    ai_cleaned_urls: List[str] = field(default_factory=list)  # è¢«AIæ¸…æ´—çš„URLåˆ—è¡¨
    
    def add_skipped_ai_cleaned(self, url: str, title: str = '') -> None:
        """è®°å½•è¢« AI æ¸…æ´—çš„ URL"""
        self.skipped_ai_cleaned += 1
        self.ai_cleaned_urls.append(f"{title[:50]}..." if title else url)
    
    def print_report(self) -> None:
        """æ‰“å°çˆ¬å–æŠ¥å‘Š"""
        logger.info("=" * 60)
        logger.info(f"ğŸ“Š çˆ¬å–æŠ¥å‘Š: {self.vendor.upper()} - {self.source_type}")
        logger.info("=" * 60)
        logger.info(f"  ğŸ” å‘ç°æ€»æ•°: {self.total_discovered}")
        logger.info(f"  âœ… æ–°å¢ä¿å­˜: {self.new_saved}")
        logger.info(f"  â­ï¸  è·³è¿‡(å·²å­˜åœ¨): {self.skipped_exists}")
        logger.info(f"  ğŸ§¹ è·³è¿‡(AIæ¸…æ´—): {self.skipped_ai_cleaned}")
        if self.failed > 0:
            logger.info(f"  âŒ å¤±è´¥æ•°: {self.failed}")
        logger.info("-" * 60)
        
        # å¦‚æœæœ‰è¢«AIæ¸…æ´—çš„è®°å½•ï¼Œæ‰“å°è¯¦ç»†åˆ—è¡¨
        if self.ai_cleaned_urls:
            logger.info("ğŸ§¹ è¢«AIæ¸…æ´—çš„è®°å½•ï¼ˆéç½‘ç»œç›¸å…³ï¼‰:")
            for i, url_or_title in enumerate(self.ai_cleaned_urls[:10], 1):
                logger.info(f"    {i}. {url_or_title}")
            if len(self.ai_cleaned_urls) > 10:
                logger.info(f"    ... å’Œå…¶ä»– {len(self.ai_cleaned_urls) - 10} æ¡")
        
        logger.info("=" * 60)

class BaseCrawler(ABC):
    """çˆ¬è™«åŸºç±»ï¼Œæä¾›åŸºç¡€çˆ¬è™«åŠŸèƒ½"""
    
    def __init__(self, config: Dict[str, Any], vendor: str, source_type: str):
        """
        åˆå§‹åŒ–çˆ¬è™«
        
        Args:
            config: é…ç½®ä¿¡æ¯
            vendor: å‚å•†åç§°ï¼ˆå¦‚aws, azureç­‰ï¼‰
            source_type: æºç±»å‹ï¼ˆå¦‚blog, docsç­‰ï¼‰
        """
        self.config = config
        self.vendor = vendor
        self.source_type = source_type
        self.crawler_config = config.get('crawler', {})
        self.timeout = self.crawler_config.get('timeout', 30)
        self.retry = self.crawler_config.get('retry', 3)
        self.interval = self.crawler_config.get('interval', 2)
        self.headers = self.crawler_config.get('headers', {})
        
        # åˆ›å»ºæ¯ä¸ªçˆ¬è™«å®ä¾‹çš„çº¿ç¨‹é”
        self.lock = threading.RLock()
        
        # åˆ›å»ºä¿å­˜ç›®å½•ï¼Œä½¿ç”¨ç›¸å¯¹äºé¡¹ç›®æ ¹ç›®å½•çš„è·¯å¾„
        base_dir = os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))
        self.output_dir = os.path.join(base_dir, 'data', 'raw', vendor, source_type)
        os.makedirs(self.output_dir, exist_ok=True)
        
        # åˆå§‹åŒ–HTMLåˆ°Markdownè½¬æ¢å™¨
        self.html_converter = self._init_html_converter()
        
        # åˆå§‹åŒ–æ–°ç»„ä»¶
        self._content_parser = content_parser
        self._date_extractor = date_extractor
        self._file_storage = FileStorage(base_dir, vendor, source_type)
        
        # åˆå§‹åŒ–å¾…åŒæ­¥çš„æ•°æ®åˆ—è¡¨ï¼ˆç”¨äºæ‰¹é‡åŒæ­¥åˆ°æ•°æ®åº“ï¼‰
        self._pending_sync_updates = {}
        
        # åˆ†æ‰¹å…¥åº“é˜ˆå€¼ï¼Œæ¯ç´¯ç§¯è¿™ä¹ˆå¤šæ¡å°±å…¥åº“ä¸€æ¬¡
        self._batch_sync_size = 50
        
        # åˆå§‹åŒ–æ•°æ®åº“å±‚ï¼ˆå»¶è¿ŸåŠ è½½ï¼‰
        self._data_layer = None
        
        # åˆå§‹åŒ–çˆ¬å–æŠ¥å‘Š
        self._crawl_report = CrawlReport(vendor=vendor, source_type=source_type)
    
    @property
    def data_layer(self):
        """å»¶è¿Ÿåˆå§‹åŒ–æ•°æ®åº“å±‚"""
        if self._data_layer is None:
            from src.storage.database.sqlite_layer import UpdateDataLayer
            self._data_layer = UpdateDataLayer()
        return self._data_layer
    
    @abstractmethod
    def _get_identifier_strategy(self) -> str:
        """
        è·å–identifierç”Ÿæˆç­–ç•¥
        
        Returns:
            'api_based': ä½¿ç”¨API base URL + èµ„æºIDï¼ˆAWS/Azureï¼‰
            'content_based': ä½¿ç”¨URL + date + product + titleï¼ˆGCP/åä¸º/è…¾è®¯äº‘/ç«å±±å¼•æ“ï¼‰
        """
        pass
    
    @abstractmethod
    def _get_identifier_components(self, update: Dict[str, Any]) -> List[str]:
        """
        è·å–ç”¨äºç”Ÿæˆidentifierçš„ç»„ä»¶
        
        Args:
            update: æ›´æ–°æ•°æ®å­—å…¸
            
        Returns:
            ç»„ä»¶åˆ—è¡¨ï¼Œç”¨äºhashç”Ÿæˆ
        """
        pass
    
    def generate_source_identifier(self, update: Dict[str, Any]) -> str:
        """
        ç»Ÿä¸€çš„source_identifierç”Ÿæˆæ–¹æ³•
        
        Args:
            update: æ›´æ–°æ•°æ®å­—å…¸
            
        Returns:
            12ä½å°å†™åå…­è¿›åˆ¶hashå­—ç¬¦ä¸²
        """
        components = self._get_identifier_components(update)
        content = '|'.join(str(c) for c in components)
        return hashlib.md5(content.encode('utf-8')).hexdigest()[:12]
    
    def should_skip_update(
        self,
        update: Dict[str, Any] = None,
        source_url: str = None,
        source_identifier: str = None,
        title: str = ''
    ) -> Tuple[bool, str]:
        """
        ç»Ÿä¸€å»é‡æ£€æŸ¥ - æ”¯æŒä¸¤ç§è°ƒç”¨æ–¹å¼
        
        æ£€æŸ¥é¡ºåº:
        1. æ£€æŸ¥æ•°æ®åº“æ˜¯å¦å·²å­˜åœ¨
        2. æ£€æŸ¥æ˜¯å¦å·²è¢« AI æ¸…æ´—ï¼ˆéç½‘ç»œç›¸å…³å·²åˆ é™¤ï¼‰
        
        è°ƒç”¨æ–¹å¼:
        1. should_skip_update(source_url=url, source_identifier=id, title=title)
        2. should_skip_update(update=update_dict)
        
        Args:
            update: å®Œæ•´çš„æ›´æ–°å­—å…¸ï¼ˆPattern 2 çˆ¬è™«ä½¿ç”¨ï¼‰
            source_url: æºURLï¼ˆPattern 1 çˆ¬è™«ä½¿ç”¨ï¼‰
            source_identifier: æºæ ‡è¯†ç¬¦ï¼ˆPattern 1 çˆ¬è™«ä½¿ç”¨ï¼‰
            title: æ ‡é¢˜ï¼ˆç”¨äºæ—¥å¿—ï¼‰
            
        Returns:
            (should_skip, reason) å…ƒç»„
            - should_skip: æ˜¯å¦åº”è·³è¿‡
            - reason: 'exists' | 'ai_cleaned' | ''
        """
        # å¦‚æœä¼ äº† update å­—å…¸ï¼Œä»ä¸­æå–å‚æ•°
        if update is not None:
            source_url = update.get('source_url', '')
            source_identifier = update.get('source_identifier') or self.generate_source_identifier(update)
            title = update.get('title', '')
        
        # å‚æ•°æ ¡éªŒ
        if not source_url:
            return False, ''
        
        # 1. æ£€æŸ¥æ•°æ®åº“æ˜¯å¦å·²å­˜åœ¨
        if self.data_layer.check_update_exists(source_url, source_identifier or ''):
            self._crawl_report.skipped_exists += 1
            return True, 'exists'
        
        # 2. æ£€æŸ¥æ˜¯å¦è¢«AIæ¸…æ´—è¿‡
        if self.data_layer.check_cleaned_by_ai(source_url):
            self._crawl_report.add_skipped_ai_cleaned(source_url, title)
            return True, 'ai_cleaned'
        
        return False, ''
    
    def save_update(self, update: Dict[str, Any]) -> bool:
        """
        ä¿å­˜æ›´æ–°æ•°æ®ï¼ˆå…¥åº“ + ä¿å­˜æ–‡ä»¶ï¼‰
        
        Args:
            update: æ›´æ–°æ•°æ®å­—å…¸ï¼Œå¿…é¡»åŒ…å«ï¼š
                - source_url: æºURL
                - source_identifier: æºæ ‡è¯†ç¬¦ï¼ˆå¦‚æœæ²¡æœ‰ä¼šè‡ªåŠ¨ç”Ÿæˆï¼‰
                - publish_date: å‘å¸ƒæ—¥æœŸ
                - title: æ ‡é¢˜
                - content: å†…å®¹ï¼ˆå¦‚æœæ²¡æœ‰ï¼Œä¼šç”¨ description å¡«å……ï¼‰
                å¯é€‰å­—æ®µï¼š
                - description: æè¿°
                - product_name: äº§å“åç§°
                
        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        try:
            # ç¡®ä¿æœ‰ source_identifier
            if not update.get('source_identifier'):
                update['source_identifier'] = self.generate_source_identifier(update)
            
            source_identifier = update['source_identifier']
            
            # ç»Ÿä¸€æ—¥æœŸæ ¼å¼: YYYY-MM -> YYYY-MM-01
            publish_date = update.get('publish_date', '')
            if publish_date and len(publish_date) == 7:  # YYYY-MM æ ¼å¼
                publish_date = f"{publish_date}-01"
                update['publish_date'] = publish_date
            
            # è·å– contentï¼ˆä¸è‡ªåŠ¨ç”¨ description å¡«å……ï¼Œè®© _export_to_file ç»Ÿä¸€å¤„ç†ï¼‰
            content = update.get('content', '')
            
            # ä¿å­˜åŸå§‹æ–‡ä»¶
            filepath = self._export_to_file(update, content)
            
            # sync_entry çš„ content å­—æ®µï¼šä¼˜å…ˆç”¨ contentï¼Œå¦åˆ™ç”¨ description
            sync_content = content or update.get('description', '')
            
            # åˆ›å»ºåŒæ­¥æ¡ç›®
            sync_entry = {
                'title': update.get('title', ''),
                'publish_date': update.get('publish_date', ''),
                'source_url': update.get('source_url', ''),
                'source_identifier': source_identifier,
                'content': sync_content,
                'description': update.get('description', ''),
                'product_name': update.get('product_name', ''),
                'crawl_time': datetime.datetime.now().isoformat(),
                'file_hash': hashlib.md5(sync_content.encode('utf-8')).hexdigest(),
                'vendor': self.vendor,
                'source_type': self.source_type,
                'filepath': filepath  # æ·»åŠ æ–‡ä»¶è·¯å¾„
            }
            
            # æ”¶é›†å¾…åŒæ­¥æ•°æ®
            self._pending_sync_updates[source_identifier] = sync_entry
            
            # è¾¾åˆ°é˜ˆå€¼æ—¶è‡ªåŠ¨å…¥åº“
            if len(self._pending_sync_updates) >= self._batch_sync_size:
                logger.info(f"å·²ç´¯ç§¯ {len(self._pending_sync_updates)} æ¡ï¼Œæ‰§è¡Œåˆ†æ‰¹å…¥åº“...")
                self.batch_sync_to_database()  # è£…é¥°å™¨ä¼šæ¸…ç©º _pending_sync_updates
            
            logger.debug(f"å·²æ”¶é›†æ›´æ–°: {update.get('title', '')[:30]}...")
            return True
            
        except Exception as e:
            logger.error(f"ä¿å­˜æ›´æ–°å¤±è´¥: {e}")
            return False
    
    def save_update_file(self, update: Dict[str, Any], markdown_content: str) -> Optional[str]:
        """
        [å·²åºŸå¼ƒ] ç»Ÿä¸€çš„æ–‡ä»¶ä¿å­˜æ–¹æ³•ï¼Œè¯·ä½¿ç”¨ save_update() ä»£æ›¿
        
        ä¿ç•™æ­¤æ–¹æ³•ä»…ä¸ºå‘åå…¼å®¹ï¼Œå†…éƒ¨è°ƒç”¨ save_update()
        """
        # å°† markdown_content ä½œä¸º content
        update['content'] = markdown_content
        
        # è°ƒç”¨æ–°æ–¹æ³•
        success = self.save_update(update)
        
        if success:
            # å¯é€‰ï¼šå¯¼å‡ºæ–‡ä»¶
            return self._export_to_file(update, markdown_content)
        return None
    
    def _export_to_file(self, update: Dict[str, Any], content: str) -> Optional[str]:
        """
        å¯¼å‡ºæ›´æ–°å†…å®¹åˆ°æ–‡ä»¶ï¼ˆåŒ…å«å…ƒæ•°æ®å¤´ï¼‰
        
        Args:
            update: æ›´æ–°æ•°æ®
            content: å†…å®¹
            
        Returns:
            æ–‡ä»¶è·¯å¾„
        """
        try:
            source_url = update.get('source_url', '')
            publish_date = update.get('publish_date', '')
            title = update.get('title', 'æ— æ ‡é¢˜')
            product_name = update.get('product_name', '')
            
            url_hash = hashlib.md5(source_url.encode('utf-8')).hexdigest()[:8]
            filename = f"{publish_date}_{url_hash}.md"
            filepath = os.path.join(self.output_dir, filename)
            
            # æ„å»ºå¸¦å…ƒæ•°æ®å¤´çš„å†…å®¹
            metadata_lines = [
                f"# {title}",
                "",
                f"**å‘å¸ƒæ—¶é—´:** {publish_date}",
                "",
                f"**å‚å•†:** {self.vendor.upper()}",
                "",
            ]
            
            if product_name:
                metadata_lines.extend([
                    f"**äº§å“:** {product_name}",
                    "",
                ])
            
            metadata_lines.extend([
                f"**ç±»å‹:** {self.source_type}",
                "",
                f"**åŸå§‹é“¾æ¥:** {source_url}",
                "",
                "---",
                "",
            ])
            
            # ç»„è£…æ­£æ–‡å†…å®¹ï¼ˆå¥—ç”¨ description/stage/doc_links ç­‰æ‰©å±•å­—æ®µï¼‰
            body_parts = []
            
            # å¦‚æœæœ‰ contentï¼Œç›´æ¥ä½¿ç”¨
            if content:
                body_parts.append(content)
            else:
                # å¦åˆ™ç»„è£… description/stage/doc_links
                description = update.get('description', '')
                stage = update.get('stage', '')
                doc_links = update.get('doc_links', [])
                
                if description:
                    body_parts.append("## å†…å®¹æè¿°\n")
                    body_parts.append(description)
                
                if stage:
                    body_parts.append("\n## å‘å¸ƒé˜¶æ®µ\n")
                    body_parts.append(stage)
                
                if doc_links:
                    body_parts.append("\n## ç›¸å…³æ–‡æ¡£\n")
                    for doc_link in doc_links:
                        body_parts.append(f"- [{doc_link.get('text', '')}]({doc_link.get('url', '')})")
            
            final_content = "\n".join(metadata_lines) + '\n'.join(body_parts)
            
            os.makedirs(self.output_dir, exist_ok=True)
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(final_content)
            
            return filepath
        except Exception as e:
            logger.error(f"å¯¼å‡ºæ–‡ä»¶å¤±è´¥: {e}")
            return None
    
    def _close_driver(self) -> None:
        """å…³é—­WebDriverï¼ˆå·²åºŸå¼ƒï¼Œä¿ç•™ç©ºæ–¹æ³•ä»¥å…¼å®¹ç°æœ‰ä»£ç ï¼‰"""
        pass
    
    def _get_http(self, url: str) -> Optional[str]:
        """
        ä½¿ç”¨requestsè·å–ç½‘é¡µå†…å®¹
        
        Args:
            url: ç›®æ ‡URL
            
        Returns:
            ç½‘é¡µHTMLå†…å®¹æˆ–Noneï¼ˆå¦‚æœå¤±è´¥ï¼‰
        """
        for i in range(self.retry):
            try:
                response = requests.get(url, headers=self.headers, timeout=self.timeout)
                response.raise_for_status()
                return response.text
            except Exception as e:
                logger.warning(f"HTTPè¯·æ±‚å¤±è´¥ (å°è¯• {i+1}/{self.retry}): {url} - {e}")
                if i < self.retry - 1:
                    time.sleep(self.interval)
        
        return None
    
    def _init_html_converter(self):
        """
        åˆå§‹åŒ–HTMLåˆ°Markdownè½¬æ¢å™¨
        
        Returns:
            HTML2Textå¯¹è±¡æˆ–None
        """
        if HTML2TEXT_AVAILABLE:
            converter = html2text.HTML2Text()
            converter.ignore_links = False
            converter.ignore_images = False
            converter.ignore_tables = False
            converter.body_width = 0  # ä¸é™åˆ¶å®½åº¦
            converter.use_automatic_links = True  # ä½¿ç”¨è‡ªåŠ¨é“¾æ¥
            converter.emphasis_mark = '*'  # å¼ºè°ƒä½¿ç”¨æ˜Ÿå·
            converter.strong_mark = '**'  # åŠ ç²—ä½¿ç”¨åŒæ˜Ÿå·
            converter.wrap_links = False  # ä¸æ¢è¡Œé“¾æ¥
            converter.pad_tables = True  # è¡¨æ ¼å¡«å……
            return converter
        return None
    
    def save_to_markdown(self, url: str, title: str, content_and_date: Tuple[str, Optional[str]], metadata_extra: Dict[str, Any] = None, batch_mode: bool = True) -> str:
        """
        [å·²åºŸå¼ƒ] å°†çˆ¬å–çš„å†…å®¹ä¿å­˜ä¸ºMarkdownæ–‡ä»¶ï¼Œè¯·ä½¿ç”¨ save_update() ä»£æ›¿
        
        ä¿ç•™æ­¤æ–¹æ³•ä»…ä¸ºå‘åå…¼å®¹
        """
        content, pub_date = content_and_date
        
        if not pub_date:
            pub_date = datetime.datetime.now().strftime("%Y_%m_%d")
        
        # æ„å»º update å¯¹è±¡
        update = {
            'source_url': url,
            'title': title,
            'content': content,
            'publish_date': pub_date.replace('_', '-') if pub_date else '',
            'vendor': self.vendor,
            'source_type': self.source_type
        }
        if metadata_extra:
            update.update(metadata_extra)
        
        # è°ƒç”¨æ–°æ–¹æ³•
        with self.lock:
            self.save_update(update)
            
            # å¯é€‰ï¼šå¯¼å‡ºæ–‡ä»¶
            # æ„å»ºMarkdownå†…å®¹
            display_date = pub_date.replace('_', '-') if pub_date else "æœªçŸ¥"
            metadata_lines = [
                f"# {title}",
                "",
                f"**åŸå§‹é“¾æ¥:** [{url}]({url})",
                "",
                f"**å‘å¸ƒæ—¶é—´:** {display_date}",
                "",
                f"**å‚å•†:** {self.vendor.upper()}",
                "",
                f"**ç±»å‹:** {self.source_type.upper()}",
                "",
                "---",
                "",
            ]
            final_content = "\n".join(metadata_lines) + content
            file_path = self._export_to_file(update, final_content)
        
        return file_path
    
    def _create_filename(self, url: str, pub_date: str, ext: str) -> str:
        """
        æ ¹æ®å‘å¸ƒæ—¥æœŸå’ŒURLå“ˆå¸Œå€¼åˆ›å»ºæ–‡ä»¶å
        
        Args:
            url: æ–‡ç« URL
            pub_date: å‘å¸ƒæ—¥æœŸï¼ˆYYYY_MM_DDæ ¼å¼ï¼‰
            ext: æ–‡ä»¶æ‰©å±•åï¼ˆå¦‚.mdï¼‰
            
        Returns:
            æ ¼å¼ä¸º: YYYY_MM_DD_URLHASH.md çš„æ–‡ä»¶å
        """
        # ç”ŸæˆURLçš„å“ˆå¸Œå€¼ï¼ˆå–å‰8ä½ä½œä¸ºçŸ­å“ˆå¸Œï¼‰
        url_hash = hashlib.md5(url.encode()).hexdigest()[:8]
        
        # ç»„åˆæ—¥æœŸå’Œå“ˆå¸Œå€¼
        filename = f"{pub_date}_{url_hash}{ext}"
        
        return filename
    
    def _extract_publish_date(self, soup: BeautifulSoup, list_date: Optional[str] = None, url: str = None) -> str:
        """
        ä»æ–‡ç« ä¸­æå–å‘å¸ƒæ—¥æœŸ
        
        Args:
            soup: BeautifulSoupå¯¹è±¡
            list_date: ä»åˆ—è¡¨é¡µè·å–çš„æ—¥æœŸï¼ˆå¯é€‰ï¼‰
            url: æ–‡ç« URLï¼ˆå¯é€‰ï¼‰
            
        Returns:
            å‘å¸ƒæ—¥æœŸå­—ç¬¦ä¸² (YYYY_MM_DDæ ¼å¼)ï¼Œå¦‚æœæ‰¾ä¸åˆ°åˆ™è¿”å›None
        """
        date_format = "%Y_%m_%d"
        
        # ç‰¹åˆ«é’ˆå¯¹åšå®¢çš„æ—¥æœŸæå– - ä¼˜å…ˆæ£€æŸ¥timeæ ‡ç­¾
        time_elements = soup.find_all('time')
        if time_elements:
            for time_elem in time_elements:
                # æ£€æŸ¥å…·æœ‰datePublishedå±æ€§çš„timeæ ‡ç­¾
                if time_elem.get('property') == 'datePublished' and time_elem.get('datetime'):
                    datetime_str = time_elem.get('datetime')
                    try:
                        # å¤„ç†ISOæ ¼å¼çš„æ—¥æœŸæ—¶é—´ "2025-04-08T17:34:26-07:00"
                        # ä»datetimeå±æ€§ä¸­æå–æ—¥æœŸéƒ¨åˆ†
                        date_part = datetime_str.split('T')[0]
                        parsed_date = datetime.datetime.strptime(date_part, '%Y-%m-%d')
                        logging.info(f"ä»timeæ ‡ç­¾çš„datetimeå±æ€§è§£æåˆ°æ—¥æœŸ: {parsed_date.strftime(date_format)}")
                        return parsed_date.strftime(date_format)
                    except (ValueError, IndexError) as e:
                        logging.debug(f"è§£ætimeæ ‡ç­¾çš„datetimeå±æ€§å¤±è´¥: {e}")
                
                # å¦‚æœæ²¡æœ‰datetimeå±æ€§æˆ–è§£æå¤±è´¥ï¼Œå°è¯•è§£ææ ‡ç­¾æ–‡æœ¬
                date_text = time_elem.get_text().strip()
                if date_text:
                    try:
                        # å°è¯•è§£æ "08 APR 2025" æ ¼å¼
                        parsed_date = datetime.datetime.strptime(date_text, '%d %b %Y')
                        logging.info(f"ä»timeæ ‡ç­¾çš„æ–‡æœ¬å†…å®¹è§£æåˆ°æ—¥æœŸ: {parsed_date.strftime(date_format)}")
                        return parsed_date.strftime(date_format)
                    except ValueError:
                        try:
                            # å°è¯•è§£æ "April 08, 2025" æ ¼å¼
                            parsed_date = datetime.datetime.strptime(date_text, '%B %d, %Y')
                            logging.info(f"ä»timeæ ‡ç­¾çš„æ–‡æœ¬å†…å®¹è§£æåˆ°æ—¥æœŸ: {parsed_date.strftime(date_format)}")
                            return parsed_date.strftime(date_format)
                        except ValueError:
                            continue

        # æŸ¥æ‰¾å…ƒæ•°æ®ä¸­çš„æ—¥æœŸ
        meta_published = soup.find('meta', property='article:published_time') or soup.find('meta', property='publish_date')
        if meta_published and meta_published.get('content'):
            try:
                content = meta_published.get('content')
                # å¤„ç†ISOæ ¼å¼æ—¥æœŸ
                if 'T' in content:
                    date_part = content.split('T')[0]
                    parsed_date = datetime.datetime.strptime(date_part, '%Y-%m-%d')
                else:
                    parsed_date = datetime.datetime.strptime(content, '%Y-%m-%d')
                logging.info(f"ä»metaæ ‡ç­¾è§£æåˆ°æ—¥æœŸ: {parsed_date.strftime(date_format)}")
                return parsed_date.strftime(date_format)
            except (ValueError, IndexError) as e:
                logging.debug(f"è§£æmetaæ ‡ç­¾æ—¥æœŸå¤±è´¥: {e}")
        
        # å°è¯•ä¸åŒçš„é€‰æ‹©å™¨æ¥å®šä½æ—¥æœŸå…ƒç´ 
        date_selectors = [
            '.lb-blog-header__date', '.blog-date', '.date', '.published-date', '.post-date',
            '.post-meta time', '.post-meta .date', '.entry-date', '.meta-date',
            'time', '[itemprop="datePublished"]', '.aws-blog-post-date', '.aws-date'
        ]
        
        # éå†æ‰€æœ‰å¯èƒ½çš„é€‰æ‹©å™¨
        for selector in date_selectors:
            date_elements = soup.select(selector)
            
            if date_elements:
                for date_elem in date_elements:
                    # å°è¯•è·å–datetimeå±æ€§
                    date_str = date_elem.get('datetime') or date_elem.text.strip()
                    if date_str:
                        try:
                            # å°è¯•å¤šç§æ—¥æœŸæ ¼å¼
                            for date_pattern in [
                                '%Y-%m-%d', '%Y/%m/%d', '%b %d, %Y', '%B %d, %Y',
                                '%d %b %Y', '%d %B %Y', '%m/%d/%Y', '%d-%m-%Y',
                                '%Yå¹´%mæœˆ%dæ—¥', '%Y.%m.%d'
                            ]:
                                try:
                                    # æå–æ—¥æœŸå­—ç¬¦ä¸²
                                    # å¦‚æœå­—ç¬¦ä¸²ä¸­åŒ…å«æ—¶é—´ï¼Œåªä¿ç•™æ—¥æœŸéƒ¨åˆ†
                                    if ' ' in date_str and not any(month in date_str for month in ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec', 'January', 'February', 'March', 'April', 'June', 'July', 'August', 'September', 'October', 'November', 'December']):
                                        date_str = date_str.split(' ')[0]
                                    
                                    parsed_date = datetime.datetime.strptime(date_str, date_pattern)
                                    logging.info(f"ä»é€‰æ‹©å™¨ {selector} è§£æåˆ°æ—¥æœŸ: {parsed_date.strftime(date_format)}")
                                    return parsed_date.strftime(date_format)
                                except ValueError:
                                    continue
                        except Exception as e:
                            logging.debug(f"æ—¥æœŸè§£æé”™è¯¯: {e}")
        
        # å¦‚æœé€šè¿‡é€‰æ‹©å™¨æ²¡æ‰¾åˆ°ï¼Œå°è¯•åœ¨æ–‡æœ¬ä¸­æœç´¢æ—¥æœŸæ¨¡å¼
        try:
            text = soup.get_text()
            
            # å¸¸è§æ—¥æœŸæ ¼å¼çš„æ­£åˆ™è¡¨è¾¾å¼
            date_patterns = [
                # YYYY-MM-DD
                r'(\d{4}-\d{1,2}-\d{1,2})',
                # YYYY/MM/DD
                r'(\d{4}/\d{1,2}/\d{1,2})',
                # Month DD, YYYY
                r'(January|February|March|April|May|June|July|August|September|October|November|December)\s+\d{1,2},\s+\d{4}',
                r'(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\s+\d{1,2},\s+\d{4}',
                # DD Month YYYY
                r'\d{1,2}\s+(January|February|March|April|May|June|July|August|September|October|November|December)\s+\d{4}',
                r'\d{1,2}\s+(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\s+\d{4}',
                # MM/DD/YYYY
                r'(\d{1,2}/\d{1,2}/\d{4})',
            ]
            
            for pattern in date_patterns:
                matches = re.search(pattern, text)
                if matches:
                    date_str = matches.group(0)
                    try:
                        # å°è¯•è§£ææ‰¾åˆ°çš„æ—¥æœŸ
                        if '-' in date_str:
                            parsed_date = datetime.datetime.strptime(date_str, '%Y-%m-%d')
                        elif '/' in date_str:
                            # å°è¯•ä¸¤ç§ä¸åŒçš„æ—¥æœŸæ ¼å¼ï¼ˆYYYY/MM/DD æˆ– MM/DD/YYYYï¼‰
                            try:
                                parsed_date = datetime.datetime.strptime(date_str, '%Y/%m/%d')
                            except ValueError:
                                parsed_date = datetime.datetime.strptime(date_str, '%m/%d/%Y')
                        elif ',' in date_str:
                            try:
                                parsed_date = datetime.datetime.strptime(date_str, '%B %d, %Y')
                            except ValueError:
                                parsed_date = datetime.datetime.strptime(date_str, '%b %d, %Y')
                        else:
                            try:
                                parsed_date = datetime.datetime.strptime(date_str, '%d %B %Y')
                            except ValueError:
                                parsed_date = datetime.datetime.strptime(date_str, '%d %b %Y')
                        
                        logging.info(f"ä»æ–‡æœ¬å†…å®¹è§£æåˆ°æ—¥æœŸ: {parsed_date.strftime(date_format)}")
                        return parsed_date.strftime(date_format)
                    except ValueError:
                        continue
        except Exception as e:
            logging.debug(f"ä»æ–‡æœ¬æå–æ—¥æœŸé”™è¯¯: {e}")
        
        # å¦‚æœä»æ–‡ç« ä¸­æ²¡æœ‰æ‰¾åˆ°æ—¥æœŸï¼Œä½¿ç”¨ä»åˆ—è¡¨é¡µè·å–çš„æ—¥æœŸ
        if list_date:
            logging.info(f"ä½¿ç”¨ä»åˆ—è¡¨é¡µè·å–çš„æ—¥æœŸ: {list_date}")
            return list_date
            
        # å¦‚æœä»URLä¸­å¯»æ‰¾å¯èƒ½çš„æ—¥æœŸæ¨¡å¼
        if url:
            url_date_match = re.search(r'/(\d{4})/(\d{1,2})/(\d{1,2})/', url)
            if url_date_match:
                try:
                    year, month, day = url_date_match.groups()
                    parsed_date = datetime.datetime(int(year), int(month), int(day))
                    logging.info(f"ä»URLæå–åˆ°æ—¥æœŸ: {parsed_date.strftime(date_format)}")
                    return parsed_date.strftime(date_format)
                except (ValueError, TypeError) as e:
                    logging.debug(f"ä»URLæå–æ—¥æœŸå‡ºé”™: {e}")
        
        # å¦‚æœæ‰¾ä¸åˆ°æ—¥æœŸï¼Œä½¿ç”¨å½“å‰æ—¥æœŸ
        logging.warning("æœªæ‰¾åˆ°å‘å¸ƒæ—¥æœŸï¼Œä½¿ç”¨å½“å‰æ—¥æœŸ")
        return datetime.datetime.now().strftime(date_format)
    
    def _html_to_markdown(self, html_content: str) -> str:
        """
        å°†HTMLè½¬æ¢ä¸ºMarkdown
        
        Args:
            html_content: HTMLå†…å®¹
            
        Returns:
            Markdownå†…å®¹
        """
        if self.html_converter:
            markdown_content = self.html_converter.handle(html_content)
        else:
            # ç®€å•çš„HTMLåˆ°æ–‡æœ¬è½¬æ¢
            soup = BeautifulSoup(html_content, 'lxml')
            markdown_content = soup.get_text("\n\n", strip=True)
        
        # æ¸…ç†Markdown
        markdown_content = self._clean_markdown(markdown_content)
        
        return markdown_content
    
    def _clean_markdown(self, markdown_text: str) -> str:
        """
        æ¸…ç†Markdownæ–‡æœ¬ï¼Œå»é™¤å¤šä½™å†…å®¹å¹¶ç¾åŒ–æ ¼å¼
        
        Args:
            markdown_text: åŸå§‹Markdownæ–‡æœ¬
            
        Returns:
            æ¸…ç†åçš„Markdownæ–‡æœ¬
        """
        # å»é™¤è¿ç»­å¤šä¸ªç©ºè¡Œ
        markdown_text = re.sub(r'\n{3,}', '\n\n', markdown_text)
        
        # ç¾åŒ–ä»£ç å—
        markdown_text = re.sub(r'```([^`]+)```', r'\n\n```\1```\n\n', markdown_text)
        
        # ç¾åŒ–å›¾ç‰‡æ ¼å¼ï¼Œç¡®ä¿å›¾ç‰‡å‰åæœ‰ç©ºè¡Œ
        markdown_text = re.sub(r'([^\n])!\[', r'\1\n\n![', markdown_text)
        markdown_text = re.sub(r'\.((?:jpg|jpeg|png|gif|webp|svg))\)([^\n])', r'.\1)\n\n\2', markdown_text)
        
        return markdown_text
    
    def _is_likely_blog_post(self, url: str) -> bool:
        """
        åˆ¤æ–­URLæ˜¯å¦å¯èƒ½æ˜¯åšå®¢æ–‡ç« 
        
        Args:
            url: è¦æ£€æŸ¥çš„URL
            
        Returns:
            Trueå¦‚æœURLå¯èƒ½æ˜¯åšå®¢æ–‡ç« ï¼Œå¦åˆ™False
        """
        # ç§»é™¤åè®®å’ŒåŸŸåéƒ¨åˆ†
        parsed = urlparse(url)
        path = parsed.path
        
        # åšå®¢æ–‡ç« URLçš„å¸¸è§æ¨¡å¼
        blog_patterns = [
            r'/blogs/[^/]+/[^/]+',  # å¦‚ /blogs/networking-and-content-delivery/article-name
            r'/blog/[^/]+',         # å¦‚ /blog/article-name
            r'/post/[^/]+',         # å¦‚ /post/article-name
            r'/\d{4}/\d{2}/[^/]+',  # å¦‚ /2022/01/article-name (æ—¥æœŸæ ¼å¼)
            r'/news/[^/]+',         # å¦‚ /news/article-name
            r'/announcements/[^/]+', # å¦‚ /announcements/article-name
        ]
        
        # æ£€æŸ¥æ˜¯å¦åŒ¹é…ä»»ä½•åšå®¢æ–‡ç« æ¨¡å¼
        for pattern in blog_patterns:
            if re.search(pattern, path):
                return True
        
        # æ’é™¤æ˜æ˜¾çš„éæ–‡ç« é¡µé¢
        exclude_patterns = [
            r'/tag/', r'/tags/', r'/category/', r'/categories/',
            r'/author/', r'/about/', r'/contact/', r'/feed/',
            r'/archive/', r'/archives/', r'/page/\d+', r'/search/'
        ]
        
        for pattern in exclude_patterns:
            if re.search(pattern, path):
                return False
                
        # æ£€æŸ¥æ˜¯å¦åœ¨URLè·¯å¾„ä¸­åŒ…å«ç‰¹å®šå…³é”®è¯
        blog_keywords = ['post', 'article', 'blog', 'news', 'announcement']
        for keyword in blog_keywords:
            if keyword in path.lower():
                return True
                
        # é»˜è®¤è¿”å›Falseï¼Œå®å¯é”™è¿‡ä¹Ÿä¸è¦è¯¯æŠ¥
        return False
    
    def process_articles_in_batches(self, article_info: List[Tuple], batch_size: int = 10) -> List[str]:
        """
        åˆ†æ‰¹å¤„ç†æ–‡ç« ï¼Œå‡å°‘é”çš„ç«äº‰å’Œæ–‡ä»¶å†™å…¥æ¬¡æ•°
        
        Args:
            article_info: æ–‡ç« ä¿¡æ¯åˆ—è¡¨ï¼Œæ¯é¡¹ä¸º(æ ‡é¢˜, URL, æ—¥æœŸ)å…ƒç»„
            batch_size: æ¯æ‰¹å¤„ç†çš„æ–‡ç« æ•°é‡ï¼Œé»˜è®¤ä¸º10
            
        Returns:
            ä¿å­˜çš„æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        """
        saved_files = []
        
        # æ£€æŸ¥æ˜¯å¦å¯ç”¨äº†å¼ºåˆ¶æ¨¡å¼
        force_mode = self.crawler_config.get('force', False)
        if force_mode:
            logger.info("å¼ºåˆ¶æ¨¡å¼å·²å¯ç”¨ï¼Œå°†é‡æ–°çˆ¬å–æ‰€æœ‰æ–‡ç« ")
        
        # åˆ†æ‰¹å¤„ç†æ–‡ç« 
        for i in range(0, len(article_info), batch_size):
            batch = article_info[i:i+batch_size]
            logger.info(f"å¤„ç†ç¬¬ {i//batch_size + 1} æ‰¹æ–‡ç« ï¼Œå…± {len(batch)} ç¯‡")
            
            # è¿‡æ»¤å·²çˆ¬å–çš„æ–‡ç« 
            filtered_batch = []
            for title, url, list_date in batch:
                if force_mode:
                    filtered_batch.append((title, url, list_date))
                    logger.info(f"å¼ºåˆ¶æ¨¡å¼ï¼šå°†é‡æ–°çˆ¬å–æ–‡ç« : {title} ({url})")
                else:
                    temp_update = {'source_url': url}
                    source_identifier = self.generate_source_identifier(temp_update)
                    
                    should_skip, reason = self.should_skip_update(
                        source_url=url, 
                        source_identifier=source_identifier, 
                        title=title
                    )
                    if should_skip:
                        logger.debug(f"è·³è¿‡({reason}): {title}")
                    else:
                        filtered_batch.append((title, url, list_date))
            
            # å¤„ç†è¿™ä¸€æ‰¹æ–‡ç« 
            for idx, (title, url, list_date) in enumerate(filtered_batch, 1):
                try:
                    logger.info(f"æ­£åœ¨çˆ¬å–ç¬¬ {idx}/{len(filtered_batch)} ç¯‡æ–‡ç« : {title}")
                    
                    # è·å–æ–‡ç« å†…å®¹
                    article_html = self._get_article_html(url)
                    if not article_html:
                        logger.warning(f"è·å–æ–‡ç« å†…å®¹å¤±è´¥: {url}")
                        continue
                    
                    # è§£ææ–‡ç« å†…å®¹å’Œæ—¥æœŸ
                    article_content, pub_date = self._parse_article_content(url, article_html, list_date)
                    
                    # ä¿å­˜ä¸ºMarkdown
                    file_path = self.save_to_markdown(url, title, (article_content, pub_date))
                    saved_files.append(file_path)
                    logger.info(f"å·²ä¿å­˜æ–‡ç« : {title} -> {file_path}")
                    
                    # é—´éš”ä¸€æ®µæ—¶é—´å†çˆ¬å–ä¸‹ä¸€ç¯‡
                    if idx < len(filtered_batch):
                        time.sleep(self.interval)
                        
                except Exception as e:
                    logger.error(f"çˆ¬å–æ–‡ç« å¤±è´¥: {url} - {e}")
            
            # æ‰¹é‡åŒæ­¥åˆ°æ•°æ®åº“
            if self._pending_sync_updates:
                self.batch_sync_to_database()
        
        return saved_files
    
    def _get_article_html(self, url: str) -> Optional[str]:
        """
        è·å–æ–‡ç« HTMLå†…å®¹
        
        Args:
            url: æ–‡ç« URL
            
        Returns:
            æ–‡ç« HTMLå†…å®¹æˆ–Noneï¼ˆå¦‚æœå¤±è´¥ï¼‰
        """
        # å°è¯•ä½¿ç”¨requestsè·å–æ–‡ç« å†…å®¹
        try:
            logger.info(f"ä½¿ç”¨requestsåº“è·å–æ–‡ç« å†…å®¹: {url}")
            headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
            response = requests.get(url, headers=headers, timeout=self.timeout)
            if response.status_code == 200:
                logger.info("ä½¿ç”¨requestsåº“æˆåŠŸè·å–åˆ°æ–‡ç« å†…å®¹")
                return response.text
            else:
                logger.error(f"è¯·æ±‚è¿”å›éæˆåŠŸçŠ¶æ€ç : {response.status_code}")
        except Exception as e:
            logger.error(f"ä½¿ç”¨requestsåº“è·å–æ–‡ç« å¤±è´¥: {e}")
        
        return None
    
    def _parse_article_content(self, url: str, html: str, list_date: Optional[str] = None) -> Tuple[str, Optional[str]]:
        """
        ä»æ–‡ç« é¡µé¢è§£ææ–‡ç« å†…å®¹å’Œå‘å¸ƒæ—¥æœŸ
        
        Args:
            url: æ–‡ç« URL
            html: æ–‡ç« é¡µé¢HTML
            list_date: ä»åˆ—è¡¨é¡µè·å–çš„æ—¥æœŸï¼ˆå¯èƒ½ä¸ºNoneï¼‰
            
        Returns:
            (æ–‡ç« å†…å®¹, å‘å¸ƒæ—¥æœŸ)å…ƒç»„ï¼Œå¦‚æœæ‰¾ä¸åˆ°æ—¥æœŸåˆ™ä½¿ç”¨åˆ—è¡¨é¡µæ—¥æœŸæˆ–å½“å‰æ—¥æœŸ
        """
        soup = BeautifulSoup(html, 'lxml')
        
        # æå–å‘å¸ƒæ—¥æœŸ
        pub_date = self._extract_publish_date(soup, list_date, url)
        
        # æå–æ–‡ç« å†…å®¹
        article_content = self._extract_article_content(soup, url)
        
        return article_content, pub_date
    
    def _extract_article_content(self, soup: BeautifulSoup, url: str) -> str:
        """
        ä»æ–‡ç« é¡µé¢æå–æ–‡ç« å†…å®¹
        
        Args:
            soup: BeautifulSoupå¯¹è±¡
            url: æ–‡ç« URL
            
        Returns:
            Markdownæ ¼å¼çš„æ–‡ç« å†…å®¹
        """
        # å°è¯•å®šä½æ–‡ç« ä¸»ä½“å†…å®¹
        content_selectors = [
            'article', 
            '.entry-content', 
            '.post-content', 
            '.article-content', 
            '.main-content',
            '.blog-post',
            '.content-container',
            'main',
            '#main-content'
        ]
        
        article_elem = None
        for selector in content_selectors:
            elements = soup.select(selector)
            if elements:
                # é€‰æ‹©æœ€é•¿çš„å…ƒç´ ä½œä¸ºæ–‡ç« ä¸»ä½“
                article_elem = max(elements, key=lambda x: len(str(x)))
                break
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ–‡ç« ä¸»ä½“ï¼Œä½¿ç”¨é¡µé¢ä¸»ä½“
        if not article_elem:
            article_elem = soup.find('body')
            
        if not article_elem:
            logger.warning(f"æœªæ‰¾åˆ°æ–‡ç« ä¸»ä½“: {url}")
            return "æ— æ³•æå–æ–‡ç« å†…å®¹"
        
        # æ¸…ç†éå†…å®¹å…ƒç´ 
        for elem in article_elem.select('header, footer, sidebar, .sidebar, nav, .navigation, .ad, .ads, .comments, .social-share'):
            elem.decompose()
        
        # è½¬æ¢ä¸ºMarkdown
        html = str(article_elem)
        if self.html_converter:
            markdown_content = self.html_converter.handle(html)
        else:
            # ç®€å•çš„HTMLåˆ°æ–‡æœ¬è½¬æ¢
            markdown_content = article_elem.get_text("\n\n", strip=True)
        
        # æ¸…ç†å’Œç¾åŒ–Markdown
        markdown_content = self._clean_markdown(markdown_content)
        
        return markdown_content
    
    @sync_to_database_decorator
    def batch_sync_to_database(self) -> None:
        """
        æ‰¹é‡åŒæ­¥æ‰€æœ‰å¾…åŒæ­¥çš„æ•°æ®åˆ°æ•°æ®åº“
        
        åœ¨çˆ¬å–å®Œæˆåè°ƒç”¨æ­¤æ–¹æ³•ï¼Œä¸€æ¬¡æ€§åŒæ­¥æ‰€æœ‰æ”¶é›†çš„æ•°æ®
        æ³¨æ„ï¼šå®é™…åŒæ­¥ç”±è£…é¥°å™¨æ‰§è¡Œï¼Œæ­¤æ–¹æ³•ä»…ä½œä¸ºè§¦å‘å…¥å£
        """
        if not self._pending_sync_updates:
            logger.debug("æ— å¾…åŒæ­¥æ•°æ®")
            return
        
        # æ³¨æ„ï¼šä¸è¦åœ¨è¿™é‡Œæ¸…ç©ºï¼Œè®©è£…é¥°å™¨å¤„ç†å®Œåå†æ¸…ç©º
        # è£…é¥°å™¨ä¼šè¯»å– self._pending_sync_updates å¹¶æ‰§è¡Œå®é™…åŒæ­¥
    
    def should_crawl(self, url: str, source_identifier: str = '', title: str = '') -> bool:
        """
        æ£€æŸ¥æ˜¯å¦éœ€è¦çˆ¬å–æŸä¸ªURL
        
        Args:
            url: è¦æ£€æŸ¥çš„URL
            source_identifier: æºæ ‡è¯†ç¬¦
            title: æ ‡é¢˜ï¼ˆç”¨äºæ—¥å¿—ï¼‰
            
        Returns:
            True å¦‚æœéœ€è¦çˆ¬å–ï¼ŒFalse å¦‚æœä¸éœ€è¦
        """
        should_skip, reason = self.should_skip_update(
            source_url=url, 
            source_identifier=source_identifier, 
            title=title
        )
        if should_skip:
            if reason == 'exists':
                logger.debug(f"è·³è¿‡å·²çˆ¬å–: {url}")
            elif reason == 'ai_cleaned':
                logger.info(f"è·³è¿‡AIæ¸…æ´—: {title or url}")
            return False
        return True
    
    @property
    def crawl_report(self) -> CrawlReport:
        """è·å–çˆ¬å–æŠ¥å‘Š"""
        return self._crawl_report
    
    def set_total_discovered(self, count: int) -> None:
        """è®¾ç½®å‘ç°æ€»æ•°ï¼ˆåœ¨å­ç±»ä¸­è°ƒç”¨ï¼‰"""
        self._crawl_report.total_discovered = count
    
    def record_failed(self) -> None:
        """è®°å½•å¤±è´¥æ•°ï¼ˆåœ¨å­ç±»ä¸­è°ƒç”¨ï¼‰"""
        self._crawl_report.failed += 1

    def run(self) -> List[str]:
        """
        è¿è¡Œçˆ¬è™«
        
        Returns:
            ä¿å­˜çš„æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        """
        try:
            logger.info(f"å¼€å§‹çˆ¬å– {self.vendor} {self.source_type}")
            
            # æ¸…ç©ºå¾…åŒæ­¥åˆ—è¡¨
            self._pending_sync_updates = {}
            
            # é‡ç½®çˆ¬å–æŠ¥å‘Š
            self._crawl_report = CrawlReport(vendor=self.vendor, source_type=self.source_type)
            
            results = self._crawl()
            
            # æ‰¹é‡åŒæ­¥åˆ°æ•°æ®åº“
            if self._pending_sync_updates:
                logger.debug(f"å¾…åŒæ­¥æ•°æ®: {len(self._pending_sync_updates)} æ¡")
                self.batch_sync_to_database()
            
            # æ›´æ–°çˆ¬å–æŠ¥å‘Š
            self._crawl_report.new_saved = len(results)
            
            # æ‰“å°çˆ¬å–æŠ¥å‘Š
            self._crawl_report.print_report()
            
            logger.info(f"çˆ¬å–å®Œæˆ {self.vendor} {self.source_type}, å…±çˆ¬å– {len(results)} ä¸ªæ–‡ä»¶")
            return results
        except Exception as e:
            logger.error(f"çˆ¬å–å¤±è´¥ {self.vendor} {self.source_type}: {e}")
            # å³ä½¿çˆ¬å–å¤±è´¥ï¼Œä¹Ÿå°è¯•åŒæ­¥å·²æ”¶é›†çš„æ•°æ®
            if self._pending_sync_updates:
                logger.info(f"çˆ¬å–å¤±è´¥ï¼Œä½†ä»æœ‰ {len(self._pending_sync_updates)} æ¡å¾…åŒæ­¥æ•°æ®ï¼Œæ‰§è¡Œæ‰¹é‡åŒæ­¥")
                try:
                    self.batch_sync_to_database()
                except Exception as sync_e:
                    logger.error(f"æ‰¹é‡åŒæ­¥å¤±è´¥: {sync_e}")
            return []
        finally:
            self._close_driver()
    
    @abstractmethod
    def _crawl(self) -> List[str]:
        """
        å…·ä½“çˆ¬è™«é€»è¾‘ï¼Œç”±å­ç±»å®ç°
        
        Returns:
            ä¿å­˜çš„æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        """
        pass
