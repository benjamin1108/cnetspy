#!/usr/bin/env python
# -*- coding: utf-8 -*-

import logging
import os
import time
import re
import hashlib
import datetime
from abc import ABC, abstractmethod
from dataclasses import dataclass, field
from typing import Dict, Any, List, Optional, Tuple
from urllib.parse import urljoin, urlparse

from src.crawlers.common.content_parser import ContentParser, DateExtractor, content_parser
from src.crawlers.common.sync_decorator import CrawlerIntegration
from src.storage.file_storage import FileStorage
from src.storage.database.sqlite_layer import UpdateDataLayer

import requests
from bs4 import BeautifulSoup

# å°è¯•å¯¼å…¥html2text
try:
    import html2text
    HTML2TEXT_AVAILABLE = True
except ImportError:
    HTML2TEXT_AVAILABLE = False

logger = logging.getLogger(__name__)


@dataclass
class CrawlReport:
    """çˆ¬å–æŠ¥å‘Šæ•°æ®ç»“æ„"""
    vendor: str = ''
    source_type: str = ''
    total_discovered: int = 0         # æ€»å‘ç°æ•°
    new_saved: int = 0                # æ–°å¢ä¿å­˜æ•°
    skipped_exists: int = 0           # è·³è¿‡ï¼ˆå·²å­˜åœ¨ï¼‰
    skipped_ai_cleaned: int = 0       # è·³è¿‡ï¼ˆAIæ¸…æ´—è¿‡ï¼‰
    failed: int = 0                   # å¤±è´¥æ•°
    ai_cleaned_urls: List[str] = field(default_factory=list)  # è¢«AIæ¸…æ´—çš„URLåˆ—è¡¨
    
    def add_skipped_ai_cleaned(self, url: str, title: str = '') -> None:
        """è®°å½•è¢« AI æ¸…æ´—çš„ URL"""
        self.skipped_ai_cleaned += 1
        self.ai_cleaned_urls.append(f"{title[:50]}..." if title else url)
    
    def print_report(self) -> None:
        """æ‰“å°çˆ¬å–æŠ¥å‘Š"""
        logger.info("=" * 60)
        logger.info(f"ğŸ“Š çˆ¬å–æŠ¥å‘Š: {self.vendor.upper()} - {self.source_type}")
        logger.info("=" * 60)
        logger.info(f"  ğŸ” å‘ç°æ€»æ•°: {self.total_discovered}")
        logger.info(f"  âœ… æ–°å¢ä¿å­˜: {self.new_saved}")
        logger.info(f"  â­ï¸  è·³è¿‡(å·²å­˜åœ¨): {self.skipped_exists}")
        logger.info(f"  ğŸ§¹ è·³è¿‡(AIæ¸…æ´—): {self.skipped_ai_cleaned}")
        if self.failed > 0:
            logger.info(f"  âŒ å¤±è´¥æ•°: {self.failed}")
        logger.info("-" * 60)
        
        # å¦‚æœæœ‰è¢«AIæ¸…æ´—çš„è®°å½•ï¼Œæ‰“å°è¯¦ç»†åˆ—è¡¨
        if self.ai_cleaned_urls:
            logger.info("ğŸ§¹ è¢«AIæ¸…æ´—çš„è®°å½•ï¼ˆéç½‘ç»œç›¸å…³ï¼‰:")
            for i, url_or_title in enumerate(self.ai_cleaned_urls[:10], 1):
                logger.info(f"    {i}. {url_or_title}")
            if len(self.ai_cleaned_urls) > 10:
                logger.info(f"    ... å’Œå…¶ä»– {len(self.ai_cleaned_urls) - 10} æ¡")
        
        logger.info("=" * 60)

class BaseCrawler(ABC):
    """çˆ¬è™«åŸºç±»ï¼Œæä¾›åŸºç¡€çˆ¬è™«åŠŸèƒ½"""
    
    def __init__(self, config: Dict[str, Any], vendor: str, source_type: str):
        """
        åˆå§‹åŒ–çˆ¬è™«
        
        Args:
            config: é…ç½®ä¿¡æ¯
            vendor: å‚å•†åç§°ï¼ˆå¦‚aws, azureç­‰ï¼‰
            source_type: æºç±»å‹ï¼ˆå¦‚blog, docsç­‰ï¼‰
        """
        self.config = config
        self.vendor = vendor
        self.source_type = source_type
        self.crawler_config = config.get('crawler', {})
        self.timeout = self.crawler_config.get('timeout', 30)
        self.retry = self.crawler_config.get('retry', 3)
        self.interval = self.crawler_config.get('interval', 2)
        self.headers = self.crawler_config.get('headers', {})
        
        # åˆ›å»ºä¿å­˜ç›®å½•ï¼Œä½¿ç”¨ç›¸å¯¹äºé¡¹ç›®æ ¹ç›®å½•çš„è·¯å¾„
        base_dir = os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))
        self.output_dir = os.path.join(base_dir, 'data', 'raw', vendor, source_type)
        os.makedirs(self.output_dir, exist_ok=True)
        
        # åˆå§‹åŒ–å†…å®¹è§£æå™¨ï¼ˆå¤ç”¨å…¨å±€å•ä¾‹ï¼‰
        self._content_parser = content_parser
        # ä¿ç•™ html_converter å±æ€§ä»¥å…¼å®¹å­ç±»
        self.html_converter = self._content_parser.html_converter
        
        # åˆå§‹åŒ–æ–‡ä»¶å­˜å‚¨
        self._file_storage = FileStorage(base_dir, vendor, source_type)
        
        # åˆå§‹åŒ–å¾…åŒæ­¥çš„æ•°æ®åˆ—è¡¨ï¼ˆç”¨äºæ‰¹é‡åŒæ­¥åˆ°æ•°æ®åº“ï¼‰
        self._pending_sync_updates = {}
        
        # åˆ†æ‰¹å…¥åº“é˜ˆå€¼ï¼Œæ¯ç´¯ç§¯è¿™ä¹ˆå¤šæ¡å°±å…¥åº“ä¸€æ¬¡
        self._batch_sync_size = 50
        
        # åˆå§‹åŒ–æ•°æ®åº“å±‚ï¼ˆå¯åŠ¨æ—¶åŠ è½½ï¼Œç¡®ä¿ ImportError æ—©æœŸæš´éœ²ï¼‰
        self._data_layer = UpdateDataLayer()
        
        # åˆå§‹åŒ–çˆ¬è™«é›†æˆå™¨ï¼ˆç”¨äºæ‰¹é‡åŒæ­¥ï¼‰
        self._crawler_integration = CrawlerIntegration()
        
        # åˆå§‹åŒ–çˆ¬å–æŠ¥å‘Š
        self._crawl_report = CrawlReport(vendor=vendor, source_type=source_type)
    
    @property
    def data_layer(self):
        """è·å–æ•°æ®åº“å±‚"""
        return self._data_layer
    
    @abstractmethod
    def _get_identifier_strategy(self) -> str:
        """
        è·å–identifierç”Ÿæˆç­–ç•¥
        
        Returns:
            'api_based': ä½¿ç”¨API base URL + èµ„æºIDï¼ˆAWS/Azureï¼‰
            'content_based': ä½¿ç”¨URL + date + product + titleï¼ˆGCP/åä¸º/è…¾è®¯äº‘/ç«å±±å¼•æ“ï¼‰
        """
        pass
    
    @abstractmethod
    def _get_identifier_components(self, update: Dict[str, Any]) -> List[str]:
        """
        è·å–ç”¨äºç”Ÿæˆidentifierçš„ç»„ä»¶
        
        Args:
            update: æ›´æ–°æ•°æ®å­—å…¸
            
        Returns:
            ç»„ä»¶åˆ—è¡¨ï¼Œç”¨äºhashç”Ÿæˆ
        """
        pass
    
    def generate_source_identifier(self, update: Dict[str, Any]) -> str:
        """
        ç»Ÿä¸€çš„source_identifierç”Ÿæˆæ–¹æ³•
        
        Args:
            update: æ›´æ–°æ•°æ®å­—å…¸
            
        Returns:
            12ä½å°å†™åå…­è¿›åˆ¶hashå­—ç¬¦ä¸²
        """
        components = self._get_identifier_components(update)
        content = '|'.join(str(c) for c in components)
        return hashlib.md5(content.encode('utf-8')).hexdigest()[:12]
    
    def should_skip_update(
        self,
        update: Dict[str, Any] = None,
        source_url: str = None,
        source_identifier: str = None,
        title: str = ''
    ) -> Tuple[bool, str]:
        """
        ç»Ÿä¸€å»é‡æ£€æŸ¥ - æ”¯æŒä¸¤ç§è°ƒç”¨æ–¹å¼
        
        æ£€æŸ¥é¡ºåº:
        1. æ£€æŸ¥æ•°æ®åº“æ˜¯å¦å·²å­˜åœ¨
        2. æ£€æŸ¥æ˜¯å¦å·²è¢« AI æ¸…æ´—ï¼ˆéç½‘ç»œç›¸å…³å·²åˆ é™¤ï¼‰
        
        è°ƒç”¨æ–¹å¼:
        1. should_skip_update(source_url=url, source_identifier=id, title=title)
        2. should_skip_update(update=update_dict)
        
        Args:
            update: å®Œæ•´çš„æ›´æ–°å­—å…¸ï¼ˆPattern 2 çˆ¬è™«ä½¿ç”¨ï¼‰
            source_url: æºURLï¼ˆPattern 1 çˆ¬è™«ä½¿ç”¨ï¼‰
            source_identifier: æºæ ‡è¯†ç¬¦ï¼ˆPattern 1 çˆ¬è™«ä½¿ç”¨ï¼‰
            title: æ ‡é¢˜ï¼ˆç”¨äºæ—¥å¿—ï¼‰
            
        Returns:
            (should_skip, reason) å…ƒç»„
            - should_skip: æ˜¯å¦åº”è·³è¿‡
            - reason: 'exists' | 'ai_cleaned' | ''
        """
        # å¦‚æœä¼ äº† update å­—å…¸ï¼Œä»ä¸­æå–å‚æ•°
        if update is not None:
            source_url = update.get('source_url', '')
            source_identifier = update.get('source_identifier') or self.generate_source_identifier(update)
            title = update.get('title', '')
        
        # å‚æ•°æ ¡éªŒ
        if not source_url:
            return False, ''
        
        # 1. æ£€æŸ¥æ•°æ®åº“æ˜¯å¦å·²å­˜åœ¨
        if self.data_layer.check_update_exists(source_url, source_identifier or ''):
            self._crawl_report.skipped_exists += 1
            return True, 'exists'
        
        # 2. æ£€æŸ¥æ˜¯å¦è¢«AIæ¸…æ´—è¿‡
        if self.data_layer.check_cleaned_by_ai(source_url):
            self._crawl_report.add_skipped_ai_cleaned(source_url, title)
            return True, 'ai_cleaned'
        
        return False, ''
    
    def save_update(self, update: Dict[str, Any]) -> bool:
        """
        ä¿å­˜æ›´æ–°æ•°æ®ï¼ˆå…¥åº“ + ä¿å­˜æ–‡ä»¶ï¼‰
        
        Args:
            update: æ›´æ–°æ•°æ®å­—å…¸ï¼Œå¿…é¡»åŒ…å«ï¼š
                - source_url: æºURL
                - source_identifier: æºæ ‡è¯†ç¬¦ï¼ˆå¦‚æœæ²¡æœ‰ä¼šè‡ªåŠ¨ç”Ÿæˆï¼‰
                - publish_date: å‘å¸ƒæ—¥æœŸ
                - title: æ ‡é¢˜
                - content: å†…å®¹ï¼ˆå¦‚æœæ²¡æœ‰ï¼Œä¼šç”¨ description å¡«å……ï¼‰
                å¯é€‰å­—æ®µï¼š
                - description: æè¿°
                - product_name: äº§å“åç§°
                
        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        try:
            # ç¡®ä¿æœ‰ source_identifier
            if not update.get('source_identifier'):
                update['source_identifier'] = self.generate_source_identifier(update)
            
            source_identifier = update['source_identifier']
            
            # ç»Ÿä¸€æ—¥æœŸæ ¼å¼: YYYY-MM -> YYYY-MM-01
            publish_date = update.get('publish_date', '')
            if publish_date and len(publish_date) == 7:  # YYYY-MM æ ¼å¼
                publish_date = f"{publish_date}-01"
                update['publish_date'] = publish_date
            
            # è·å– contentï¼ˆä¸è‡ªåŠ¨ç”¨ description å¡«å……ï¼Œè®© _export_to_file ç»Ÿä¸€å¤„ç†ï¼‰
            content = update.get('content', '')
            
            # ä¿å­˜åŸå§‹æ–‡ä»¶
            filepath = self._export_to_file(update, content)
            
            # sync_entry çš„ content å­—æ®µï¼šä¼˜å…ˆç”¨ contentï¼Œå¦åˆ™ç”¨ description
            sync_content = content or update.get('description', '')
            
            # åˆ›å»ºåŒæ­¥æ¡ç›®
            sync_entry = {
                'title': update.get('title', ''),
                'publish_date': update.get('publish_date', ''),
                'source_url': update.get('source_url', ''),
                'source_identifier': source_identifier,
                'content': sync_content,
                'description': update.get('description', ''),
                'product_name': update.get('product_name', ''),
                'crawl_time': datetime.datetime.now().isoformat(),
                'file_hash': hashlib.md5(sync_content.encode('utf-8')).hexdigest(),
                'vendor': self.vendor,
                'source_type': self.source_type,
                'filepath': filepath  # æ·»åŠ æ–‡ä»¶è·¯å¾„
            }
            
            # æ”¶é›†å¾…åŒæ­¥æ•°æ®
            self._pending_sync_updates[source_identifier] = sync_entry
            
            # è¾¾åˆ°é˜ˆå€¼æ—¶è‡ªåŠ¨å…¥åº“
            if len(self._pending_sync_updates) >= self._batch_sync_size:
                logger.info(f"å·²ç´¯ç§¯ {len(self._pending_sync_updates)} æ¡ï¼Œæ‰§è¡Œåˆ†æ‰¹å…¥åº“...")
                self.batch_sync_to_database()  # è£…é¥°å™¨ä¼šæ¸…ç©º _pending_sync_updates
            
            logger.debug(f"å·²æ”¶é›†æ›´æ–°: {update.get('title', '')[:30]}...")
            return True
            
        except Exception as e:
            logger.error(f"ä¿å­˜æ›´æ–°å¤±è´¥: {e}")
            return False
    
    def save_update_file(self, update: Dict[str, Any], markdown_content: str) -> Optional[str]:
        """
        [å·²åºŸå¼ƒ] ç»Ÿä¸€çš„æ–‡ä»¶ä¿å­˜æ–¹æ³•ï¼Œè¯·ä½¿ç”¨ save_update() ä»£æ›¿
        
        ä¿ç•™æ­¤æ–¹æ³•ä»…ä¸ºå‘åå…¼å®¹ï¼Œå†…éƒ¨è°ƒç”¨ save_update()
        """
        # å°† markdown_content ä½œä¸º content
        update['content'] = markdown_content
        
        # è°ƒç”¨æ–°æ–¹æ³•ï¼ˆå†…éƒ¨å·²åŒ…å«æ–‡ä»¶å¯¼å‡ºï¼‰
        success = self.save_update(update)
        
        # è¿”å›æ–‡ä»¶è·¯å¾„ï¼ˆä» pending_sync_updates ä¸­è·å–ï¼‰
        if success:
            source_identifier = update.get('source_identifier')
            if source_identifier and source_identifier in self._pending_sync_updates:
                return self._pending_sync_updates[source_identifier].get('filepath')
        return None
    
    def _export_to_file(self, update: Dict[str, Any], content: str) -> Optional[str]:
        """
        å¯¼å‡ºæ›´æ–°å†…å®¹åˆ°æ–‡ä»¶ï¼ˆåŒ…å«å…ƒæ•°æ®å¤´ï¼‰
        
        Args:
            update: æ›´æ–°æ•°æ®
            content: å†…å®¹
            
        Returns:
            æ–‡ä»¶è·¯å¾„
        """
        try:
            source_url = update.get('source_url', '')
            publish_date = update.get('publish_date', '')
            title = update.get('title', 'æ— æ ‡é¢˜')
            product_name = update.get('product_name', '')
            
            url_hash = hashlib.md5(source_url.encode('utf-8')).hexdigest()[:8]
            filename = f"{publish_date}_{url_hash}.md"
            filepath = os.path.join(self.output_dir, filename)
            
            # æ„å»ºå¸¦å…ƒæ•°æ®å¤´çš„å†…å®¹
            metadata_lines = [
                f"# {title}",
                "",
                f"**å‘å¸ƒæ—¶é—´:** {publish_date}",
                "",
                f"**å‚å•†:** {self.vendor.upper()}",
                "",
            ]
            
            if product_name:
                metadata_lines.extend([
                    f"**äº§å“:** {product_name}",
                    "",
                ])
            
            metadata_lines.extend([
                f"**ç±»å‹:** {self.source_type}",
                "",
                f"**åŸå§‹é“¾æ¥:** {source_url}",
                "",
                "---",
                "",
            ])
            
            # ç»„è£…æ­£æ–‡å†…å®¹ï¼ˆå¥—ç”¨ description/stage/doc_links ç­‰æ‰©å±•å­—æ®µï¼‰
            body_parts = []
            
            # å¦‚æœæœ‰ contentï¼Œç›´æ¥ä½¿ç”¨
            if content:
                body_parts.append(content)
            else:
                # å¦åˆ™ç»„è£… description/stage/doc_links
                description = update.get('description', '')
                stage = update.get('stage', '')
                doc_links = update.get('doc_links', [])
                
                if description:
                    body_parts.append("## å†…å®¹æè¿°\n")
                    body_parts.append(description)
                
                if stage:
                    body_parts.append("\n## å‘å¸ƒé˜¶æ®µ\n")
                    body_parts.append(stage)
                
                if doc_links:
                    body_parts.append("\n## ç›¸å…³æ–‡æ¡£\n")
                    for doc_link in doc_links:
                        body_parts.append(f"- [{doc_link.get('text', '')}]({doc_link.get('url', '')})")
            
            final_content = "\n".join(metadata_lines) + '\n'.join(body_parts)
            
            os.makedirs(self.output_dir, exist_ok=True)
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(final_content)
            
            return filepath
        except Exception as e:
            logger.error(f"å¯¼å‡ºæ–‡ä»¶å¤±è´¥: {e}")
            return None
    
    def _close_driver(self) -> None:
        """å…³é—­WebDriverï¼ˆå·²åºŸå¼ƒï¼Œä¿ç•™ç©ºæ–¹æ³•ä»¥å…¼å®¹ç°æœ‰ä»£ç ï¼‰"""
        pass
    
    def _get_http(self, url: str) -> Optional[str]:
        """
        ä½¿ç”¨requestsè·å–ç½‘é¡µå†…å®¹
        
        Args:
            url: ç›®æ ‡URL
            
        Returns:
            ç½‘é¡µHTMLå†…å®¹æˆ–Noneï¼ˆå¦‚æœå¤±è´¥ï¼‰
        """
        for i in range(self.retry):
            try:
                response = requests.get(url, headers=self.headers, timeout=self.timeout)
                response.raise_for_status()
                return response.text
            except Exception as e:
                logger.warning(f"HTTPè¯·æ±‚å¤±è´¥ (å°è¯• {i+1}/{self.retry}): {url} - {e}")
                if i < self.retry - 1:
                    time.sleep(self.interval)
        
        return None
    
    
    def save_to_markdown(self, url: str, title: str, content_and_date: Tuple[str, Optional[str]], metadata_extra: Dict[str, Any] = None, batch_mode: bool = True) -> str:
        """
        [å·²åºŸå¼ƒ] å°†çˆ¬å–çš„å†…å®¹ä¿å­˜ä¸ºMarkdownæ–‡ä»¶ï¼Œè¯·ä½¿ç”¨ save_update() ä»£æ›¿
        
        ä¿ç•™æ­¤æ–¹æ³•ä»…ä¸ºå‘åå…¼å®¹
        """
        content, pub_date = content_and_date
        
        if not pub_date:
            pub_date = datetime.datetime.now().strftime("%Y_%m_%d")
        
        # æ„å»º update å¯¹è±¡
        update = {
            'source_url': url,
            'title': title,
            'content': content,
            'publish_date': pub_date.replace('_', '-') if pub_date else '',
            'vendor': self.vendor,
            'source_type': self.source_type
        }
        if metadata_extra:
            update.update(metadata_extra)
        
        # è°ƒç”¨æ–°æ–¹æ³•ï¼ˆå†…éƒ¨å·²åŒ…å«æ–‡ä»¶å¯¼å‡ºï¼‰
        self.save_update(update)
        
        # è¿”å›æ–‡ä»¶è·¯å¾„
        source_identifier = update.get('source_identifier')
        if source_identifier and source_identifier in self._pending_sync_updates:
            return self._pending_sync_updates[source_identifier].get('filepath', '')
        return ''
    
    def _create_filename(self, url: str, pub_date: str, ext: str) -> str:
        """
        æ ¹æ®å‘å¸ƒæ—¥æœŸå’ŒURLå“ˆå¸Œå€¼åˆ›å»ºæ–‡ä»¶å
        
        Args:
            url: æ–‡ç« URL
            pub_date: å‘å¸ƒæ—¥æœŸï¼ˆYYYY_MM_DDæ ¼å¼ï¼‰
            ext: æ–‡ä»¶æ‰©å±•åï¼ˆå¦‚.mdï¼‰
            
        Returns:
            æ ¼å¼ä¸º: YYYY_MM_DD_URLHASH.md çš„æ–‡ä»¶å
        """
        # ç”ŸæˆURLçš„å“ˆå¸Œå€¼ï¼ˆå–å‰8ä½ä½œä¸ºçŸ­å“ˆå¸Œï¼‰
        url_hash = hashlib.md5(url.encode()).hexdigest()[:8]
        
        # ç»„åˆæ—¥æœŸå’Œå“ˆå¸Œå€¼
        filename = f"{pub_date}_{url_hash}{ext}"
        
        return filename
    
    def _extract_publish_date(self, soup: BeautifulSoup, list_date: Optional[str] = None, url: str = None) -> str:
        """
        ä»æ–‡ç« ä¸­æå–å‘å¸ƒæ—¥æœŸ
        
        å§”æ‰˜ç»™ DateExtractor å¤„ç†ï¼Œå­ç±»å¯é‡å†™æ­¤æ–¹æ³•å®ç°ç‰¹å®šè§£æç­–ç•¥
        
        Args:
            soup: BeautifulSoupå¯¹è±¡
            list_date: ä»åˆ—è¡¨é¡µè·å–çš„æ—¥æœŸï¼ˆå¯é€‰ï¼‰
            url: æ–‡ç« URLï¼ˆå¯é€‰ï¼‰
            
        Returns:
            å‘å¸ƒæ—¥æœŸå­—ç¬¦ä¸² (YYYY_MM_DDæ ¼å¼)
        """
        return DateExtractor.extract_publish_date(soup, list_date, url)
    
    def _html_to_markdown(self, html_content: str) -> str:
        """å°†HTMLè½¬æ¢ä¸ºMarkdownï¼ˆå§”æ‰˜ç»™ ContentParserï¼‰"""
        return self._content_parser.html_to_markdown(html_content)
    
    def _clean_markdown(self, markdown_text: str) -> str:
        """æ¸…ç†Markdownæ–‡æœ¬ï¼ˆå§”æ‰˜ç»™ ContentParserï¼‰"""
        return self._content_parser.clean_markdown(markdown_text)
    
    def _is_likely_blog_post(self, url: str) -> bool:
        """åˆ¤æ–­URLæ˜¯å¦å¯èƒ½æ˜¯åšå®¢æ–‡ç« ï¼ˆå§”æ‰˜ç»™ ContentParserï¼‰"""
        return self._content_parser.is_likely_blog_post(url)
    
    def process_articles_in_batches(self, article_info: List[Tuple], batch_size: int = 10) -> List[str]:
        """
        åˆ†æ‰¹å¤„ç†æ–‡ç« ï¼Œå‡å°‘é”çš„ç«äº‰å’Œæ–‡ä»¶å†™å…¥æ¬¡æ•°
        
        Args:
            article_info: æ–‡ç« ä¿¡æ¯åˆ—è¡¨ï¼Œæ¯é¡¹ä¸º(æ ‡é¢˜, URL, æ—¥æœŸ)å…ƒç»„
            batch_size: æ¯æ‰¹å¤„ç†çš„æ–‡ç« æ•°é‡ï¼Œé»˜è®¤ä¸º10
            
        Returns:
            ä¿å­˜çš„æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        """
        saved_files = []
        
        # æ£€æŸ¥æ˜¯å¦å¯ç”¨äº†å¼ºåˆ¶æ¨¡å¼
        force_mode = self.crawler_config.get('force', False)
        if force_mode:
            logger.info("å¼ºåˆ¶æ¨¡å¼å·²å¯ç”¨ï¼Œå°†é‡æ–°çˆ¬å–æ‰€æœ‰æ–‡ç« ")
        
        # åˆ†æ‰¹å¤„ç†æ–‡ç« 
        for i in range(0, len(article_info), batch_size):
            batch = article_info[i:i+batch_size]
            logger.info(f"å¤„ç†ç¬¬ {i//batch_size + 1} æ‰¹æ–‡ç« ï¼Œå…± {len(batch)} ç¯‡")
            
            # è¿‡æ»¤å·²çˆ¬å–çš„æ–‡ç« 
            filtered_batch = []
            for title, url, list_date in batch:
                if force_mode:
                    filtered_batch.append((title, url, list_date))
                    logger.info(f"å¼ºåˆ¶æ¨¡å¼ï¼šå°†é‡æ–°çˆ¬å–æ–‡ç« : {title} ({url})")
                else:
                    temp_update = {'source_url': url}
                    source_identifier = self.generate_source_identifier(temp_update)
                    
                    should_skip, reason = self.should_skip_update(
                        source_url=url, 
                        source_identifier=source_identifier, 
                        title=title
                    )
                    if should_skip:
                        logger.debug(f"è·³è¿‡({reason}): {title}")
                    else:
                        filtered_batch.append((title, url, list_date))
            
            # å¤„ç†è¿™ä¸€æ‰¹æ–‡ç« 
            for idx, (title, url, list_date) in enumerate(filtered_batch, 1):
                try:
                    logger.info(f"æ­£åœ¨çˆ¬å–ç¬¬ {idx}/{len(filtered_batch)} ç¯‡æ–‡ç« : {title}")
                    
                    # è·å–æ–‡ç« å†…å®¹
                    article_html = self._get_article_html(url)
                    if not article_html:
                        logger.warning(f"è·å–æ–‡ç« å†…å®¹å¤±è´¥: {url}")
                        continue
                    
                    # è§£ææ–‡ç« å†…å®¹å’Œæ—¥æœŸ
                    article_content, pub_date = self._parse_article_content(url, article_html, list_date)
                    
                    # ä¿å­˜ä¸ºMarkdown
                    file_path = self.save_to_markdown(url, title, (article_content, pub_date))
                    saved_files.append(file_path)
                    logger.info(f"å·²ä¿å­˜æ–‡ç« : {title} -> {file_path}")
                    
                    # é—´éš”ä¸€æ®µæ—¶é—´å†çˆ¬å–ä¸‹ä¸€ç¯‡
                    if idx < len(filtered_batch):
                        time.sleep(self.interval)
                        
                except Exception as e:
                    logger.error(f"çˆ¬å–æ–‡ç« å¤±è´¥: {url} - {e}")
            
            # æ‰¹é‡åŒæ­¥åˆ°æ•°æ®åº“
            if self._pending_sync_updates:
                self.batch_sync_to_database()
        
        return saved_files
    
    def _get_article_html(self, url: str) -> Optional[str]:
        """
        è·å–æ–‡ç« HTMLå†…å®¹
        
        Args:
            url: æ–‡ç« URL
            
        Returns:
            æ–‡ç« HTMLå†…å®¹æˆ–Noneï¼ˆå¦‚æœå¤±è´¥ï¼‰
        """
        # å°è¯•ä½¿ç”¨requestsè·å–æ–‡ç« å†…å®¹
        try:
            logger.info(f"ä½¿ç”¨requestsåº“è·å–æ–‡ç« å†…å®¹: {url}")
            headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
            response = requests.get(url, headers=headers, timeout=self.timeout)
            if response.status_code == 200:
                logger.info("ä½¿ç”¨requestsåº“æˆåŠŸè·å–åˆ°æ–‡ç« å†…å®¹")
                return response.text
            else:
                logger.error(f"è¯·æ±‚è¿”å›éæˆåŠŸçŠ¶æ€ç : {response.status_code}")
        except Exception as e:
            logger.error(f"ä½¿ç”¨requestsåº“è·å–æ–‡ç« å¤±è´¥: {e}")
        
        return None
    
    def _get_with_playwright(self, url: str, wait_for_selector: str = None) -> Optional[str]:
        """
        ä½¿ç”¨Playwrightè·å–é¡µé¢å†…å®¹ï¼ˆå¸¦é‡è¯•å’Œæ‡’åŠ è½½å¤„ç†ï¼‰
        
        Args:
            url: ç›®æ ‡URL
            wait_for_selector: ç­‰å¾…çš„CSSé€‰æ‹©å™¨ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            ç½‘é¡µHTMLå†…å®¹æˆ–Noneï¼ˆå¦‚æœå¤±è´¥ï¼‰
        """
        from playwright.sync_api import sync_playwright
        
        for i in range(self.retry):
            try:
                logger.info(f"ä½¿ç”¨Playwrightè·å–é¡µé¢: {url}")
                with sync_playwright() as p:
                    browser = p.chromium.launch(
                        headless=True,
                        args=['--no-sandbox', '--disable-dev-shm-usage']
                    )
                    try:
                        page = browser.new_page()
                        page.set_default_timeout(30000)
                        page.goto(url, wait_until='domcontentloaded')
                        
                        # ç­‰å¾…ä¸»è¦å†…å®¹åŠ è½½
                        if wait_for_selector:
                            try:
                                page.wait_for_selector(wait_for_selector, timeout=10000)
                            except:
                                pass
                        else:
                            try:
                                page.wait_for_selector('main, article, body', timeout=10000)
                            except:
                                pass
                        
                        # æ»šåŠ¨è§¦å‘æ‡’åŠ è½½
                        page.evaluate('window.scrollTo(0, document.body.scrollHeight / 2)')
                        page.wait_for_timeout(500)
                        page.evaluate('window.scrollTo(0, document.body.scrollHeight)')
                        page.wait_for_timeout(500)
                        page.evaluate('window.scrollTo(0, 0)')
                        page.wait_for_timeout(500)
                        
                        html = page.content()
                        logger.info(f"æˆåŠŸè·å–é¡µé¢å†…å®¹ï¼Œå¤§å°: {len(html)} å­—èŠ‚")
                        return html
                    finally:
                        browser.close()
            except Exception as e:
                logger.warning(f"Playwrightè·å–é¡µé¢å¤±è´¥ (å°è¯• {i+1}/{self.retry}): {url} - {e}")
                if i < self.retry - 1:
                    retry_interval = self.interval * (i + 1)
                    logger.info(f"ç­‰å¾… {retry_interval} ç§’åé‡è¯•...")
                    time.sleep(retry_interval)
        
        return None
    
    def _parse_article_content(self, url: str, html: str, list_date: Optional[str] = None) -> Tuple[str, Optional[str]]:
        """
        ä»æ–‡ç« é¡µé¢è§£ææ–‡ç« å†…å®¹å’Œå‘å¸ƒæ—¥æœŸ
        
        Args:
            url: æ–‡ç« URL
            html: æ–‡ç« é¡µé¢HTML
            list_date: ä»åˆ—è¡¨é¡µè·å–çš„æ—¥æœŸï¼ˆå¯èƒ½ä¸ºNoneï¼‰
            
        Returns:
            (æ–‡ç« å†…å®¹, å‘å¸ƒæ—¥æœŸ)å…ƒç»„ï¼Œå¦‚æœæ‰¾ä¸åˆ°æ—¥æœŸåˆ™ä½¿ç”¨åˆ—è¡¨é¡µæ—¥æœŸæˆ–å½“å‰æ—¥æœŸ
        """
        soup = BeautifulSoup(html, 'lxml')
        
        # æå–å‘å¸ƒæ—¥æœŸ
        pub_date = self._extract_publish_date(soup, list_date, url)
        
        # æå–æ–‡ç« å†…å®¹
        article_content = self._extract_article_content(soup, url)
        
        return article_content, pub_date
    
    def _extract_article_content(self, soup: BeautifulSoup, url: str) -> str:
        """ä»æ–‡ç« é¡µé¢æå–æ–‡ç« å†…å®¹ï¼ˆå§”æ‰˜ç»™ ContentParserï¼‰"""
        return self._content_parser.extract_article_content(soup, url)
    
    def batch_sync_to_database(self) -> None:
        """
        æ‰¹é‡åŒæ­¥æ‰€æœ‰å¾…åŒæ­¥çš„æ•°æ®åˆ°æ•°æ®åº“
        
        åœ¨çˆ¬å–å®Œæˆåè°ƒç”¨æ­¤æ–¹æ³•ï¼Œä¸€æ¬¡æ€§åŒæ­¥æ‰€æœ‰æ”¶é›†çš„æ•°æ®
        """
        if not self._pending_sync_updates:
            logger.debug("æ— å¾…åŒæ­¥æ•°æ®")
            return
        
        try:
            force_mode = self.crawler_config.get('force', False)
            
            # æ˜¾å¼è°ƒç”¨æ‰¹é‡åŒæ­¥
            sync_result = self._crawler_integration.batch_sync_to_database(
                self.vendor,
                self.source_type,
                self._pending_sync_updates,
                force_update=force_mode
            )
            
            if force_mode:
                logger.info(f"æ•°æ®åº“å¼ºåˆ¶æ›´æ–°å®Œæˆ: æ›´æ–°{sync_result.get('success', 0)}, å¤±è´¥{sync_result.get('failed', 0)}")
            else:
                logger.info(f"æ•°æ®åº“åŒæ­¥å®Œæˆ: æˆåŠŸ{sync_result.get('success', 0)}, è·³è¿‡{sync_result.get('skipped', 0)}, å¤±è´¥{sync_result.get('failed', 0)}")
            
            # åŒæ­¥å®Œæˆåæ¸…ç©º
            self._pending_sync_updates = {}
            
        except Exception as e:
            logger.error(f"æ‰¹é‡åŒæ­¥åˆ°æ•°æ®åº“å¤±è´¥: {e}")
            self._pending_sync_updates = {}
    
    def should_crawl(self, url: str, source_identifier: str = '', title: str = '') -> bool:
        """
        æ£€æŸ¥æ˜¯å¦éœ€è¦çˆ¬å–æŸä¸ªURL
        
        Args:
            url: è¦æ£€æŸ¥çš„URL
            source_identifier: æºæ ‡è¯†ç¬¦
            title: æ ‡é¢˜ï¼ˆç”¨äºæ—¥å¿—ï¼‰
            
        Returns:
            True å¦‚æœéœ€è¦çˆ¬å–ï¼ŒFalse å¦‚æœä¸éœ€è¦
        """
        should_skip, reason = self.should_skip_update(
            source_url=url, 
            source_identifier=source_identifier, 
            title=title
        )
        if should_skip:
            if reason == 'exists':
                logger.debug(f"è·³è¿‡å·²çˆ¬å–: {url}")
            elif reason == 'ai_cleaned':
                logger.info(f"è·³è¿‡AIæ¸…æ´—: {title or url}")
            return False
        return True
    
    @property
    def crawl_report(self) -> CrawlReport:
        """è·å–çˆ¬å–æŠ¥å‘Š"""
        return self._crawl_report
    
    def set_total_discovered(self, count: int) -> None:
        """è®¾ç½®å‘ç°æ€»æ•°ï¼ˆåœ¨å­ç±»ä¸­è°ƒç”¨ï¼‰"""
        self._crawl_report.total_discovered = count
    
    def record_failed(self) -> None:
        """è®°å½•å¤±è´¥æ•°ï¼ˆåœ¨å­ç±»ä¸­è°ƒç”¨ï¼‰"""
        self._crawl_report.failed += 1

    def run(self) -> List[str]:
        """
        è¿è¡Œçˆ¬è™«
        
        Returns:
            ä¿å­˜çš„æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        """
        try:
            logger.info(f"å¼€å§‹çˆ¬å– {self.vendor} {self.source_type}")
            
            # æ¸…ç©ºå¾…åŒæ­¥åˆ—è¡¨
            self._pending_sync_updates = {}
            
            # é‡ç½®çˆ¬å–æŠ¥å‘Š
            self._crawl_report = CrawlReport(vendor=self.vendor, source_type=self.source_type)
            
            results = self._crawl()
            
            # æ‰¹é‡åŒæ­¥åˆ°æ•°æ®åº“
            if self._pending_sync_updates:
                logger.debug(f"å¾…åŒæ­¥æ•°æ®: {len(self._pending_sync_updates)} æ¡")
                self.batch_sync_to_database()
            
            # æ›´æ–°çˆ¬å–æŠ¥å‘Š
            self._crawl_report.new_saved = len(results)
            
            # æ‰“å°çˆ¬å–æŠ¥å‘Š
            self._crawl_report.print_report()
            
            logger.info(f"çˆ¬å–å®Œæˆ {self.vendor} {self.source_type}, å…±çˆ¬å– {len(results)} ä¸ªæ–‡ä»¶")
            return results
        except Exception as e:
            logger.error(f"çˆ¬å–å¤±è´¥ {self.vendor} {self.source_type}: {e}")
            # å³ä½¿çˆ¬å–å¤±è´¥ï¼Œä¹Ÿå°è¯•åŒæ­¥å·²æ”¶é›†çš„æ•°æ®
            if self._pending_sync_updates:
                logger.info(f"çˆ¬å–å¤±è´¥ï¼Œä½†ä»æœ‰ {len(self._pending_sync_updates)} æ¡å¾…åŒæ­¥æ•°æ®ï¼Œæ‰§è¡Œæ‰¹é‡åŒæ­¥")
                try:
                    self.batch_sync_to_database()
                except Exception as sync_e:
                    logger.error(f"æ‰¹é‡åŒæ­¥å¤±è´¥: {sync_e}")
            return []
        finally:
            self._close_driver()
    
    @abstractmethod
    def _crawl(self) -> List[str]:
        """
        å…·ä½“çˆ¬è™«é€»è¾‘ï¼Œç”±å­ç±»å®ç°
        
        Returns:
            ä¿å­˜çš„æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        """
        pass
