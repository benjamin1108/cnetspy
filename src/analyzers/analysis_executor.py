#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
åˆ†ææ‰§è¡Œå™¨ - CLIå’ŒAPIçš„å…±äº«ä¸šåŠ¡é€»è¾‘

å°è£…"åˆ†æ+ä¿å­˜+æ›´æ–°"çš„å®Œæ•´æµç¨‹ï¼Œä¾›ä»¥ä¸‹æ¨¡å—å¤ç”¨ï¼š
- scripts/analyze_updates.py (CLIå·¥å…·)
- src/api/services/analysis_service.py (APIæœåŠ¡)
"""

import json
import os
from typing import Dict, Any, Optional
from datetime import datetime
import logging
from tabulate import tabulate

logger = logging.getLogger(__name__)


class AnalysisExecutor:
    """
    åˆ†ææ‰§è¡Œå™¨
    
    èŒè´£ï¼š
    1. è°ƒç”¨ UpdateAnalyzer è¿›è¡ŒAIåˆ†æ
    2. ä¿å­˜åˆ†æç»“æœåˆ°æ–‡ä»¶ï¼ˆå¯é€‰ï¼‰
    3. æ›´æ–°æ•°æ®åº“å­—æ®µ
    4. è®°å½•è´¨é‡é—®é¢˜åˆ°æ•°æ®åº“ï¼ˆéç½‘ç»œå†…å®¹åˆ é™¤ã€subcategoryä¸ºç©ºç­‰ï¼‰
    """
    
    # å½“å‰æ‰¹æ¬¡IDï¼ˆæ‰¹é‡åˆ†ææ—¶è®¾ç½®ï¼‰
    _current_batch_id: Optional[str] = None
    
    def __init__(self, analyzer, data_layer, config: Dict[str, Any]):
        """
        åˆå§‹åŒ–æ‰§è¡Œå™¨
        
        Args:
            analyzer: UpdateAnalyzer å®ä¾‹
            data_layer: UpdateDataLayer å®ä¾‹
            config: é…ç½®å­—å…¸
                - enable_file_save: æ˜¯å¦å¯ç”¨æ–‡ä»¶ä¿å­˜ï¼ˆé»˜è®¤Trueï¼‰
                - output_base_dir: æ–‡ä»¶è¾“å‡ºæ ¹ç›®å½•ï¼ˆé»˜è®¤ 'data/analyzed'ï¼‰
                - batch_id: æ‰¹æ¬¡IDï¼ˆå¯é€‰ï¼Œæ‰¹é‡åˆ†ææ—¶ä½¿ç”¨ï¼‰
        """
        self.analyzer = analyzer
        self.data_layer = data_layer
        self.config = config
        self.logger = logging.getLogger(__name__)
        
        # æ–‡ä»¶ä¿å­˜é…ç½®
        self.enable_file_save = config.get('enable_file_save', True)
        self.output_base_dir = config.get('output_base_dir', 'data/analyzed')
        
        # è®¾ç½®æ‰¹æ¬¡ID
        batch_id = config.get('batch_id')
        if batch_id:
            AnalysisExecutor._current_batch_id = batch_id
    
    @classmethod
    def set_batch_id(cls, batch_id: str) -> None:
        """è®¾ç½®å½“å‰æ‰¹æ¬¡ID"""
        cls._current_batch_id = batch_id
    
    @classmethod
    def clear_batch_id(cls) -> None:
        """æ¸…é™¤æ‰¹æ¬¡ID"""
        cls._current_batch_id = None
    
    def execute_analysis(
        self, 
        update_data: Dict[str, Any],
        save_to_db: bool = True,
        save_to_file: Optional[bool] = None
    ) -> Optional[Dict[str, Any]]:
        """
        æ‰§è¡Œå®Œæ•´çš„åˆ†ææµç¨‹
        
        Args:
            update_data: æ›´æ–°æ•°æ®å­—å…¸
            save_to_db: æ˜¯å¦ä¿å­˜åˆ°æ•°æ®åº“ï¼ˆé»˜è®¤Trueï¼‰
            save_to_file: æ˜¯å¦ä¿å­˜åˆ°æ–‡ä»¶ï¼ˆNone=ä½¿ç”¨å…¨å±€é…ç½®ï¼‰
            
        Returns:
            åˆ†æç»“æœå­—å…¸ï¼Œå¤±è´¥è¿”å› None
            
        æµç¨‹ï¼š
            1. AIåˆ†æ
            2. æ£€æŸ¥is_network_relatedï¼ˆä¸ç›¸å…³åˆ™åˆ é™¤å¹¶è®°å½•ï¼‰
            3. æ£€æŸ¥product_subcategoryï¼ˆä¸ºç©ºåˆ™è®°å½•é—®é¢˜ï¼‰
            4. ä¿å­˜æ–‡ä»¶ï¼ˆå¯é€‰ï¼‰
            5. æ›´æ–°æ•°æ®åº“
        """
        update_id = update_data.get('update_id')
        vendor = update_data.get('vendor', '')
        title = update_data.get('title', '')
        source_url = update_data.get('source_url', '')
        
        try:
            # 1. æ‰§è¡ŒAIåˆ†æ
            result = self.analyzer.analyze(update_data)
            if not result:
                self.logger.error(f"AIåˆ†æå¤±è´¥: {update_id}")
                # è®°å½•åˆ†æå¤±è´¥
                self._record_quality_issue(
                    update_id=update_id,
                    issue_type='analysis_failed',
                    auto_action='kept',
                    vendor=vendor,
                    title=title,
                    source_url=source_url
                )
                return None
            
            # 2. æ£€æŸ¥æ˜¯å¦ä¸ç½‘ç»œç›¸å…³ï¼Œä¸ç›¸å…³åˆ™åˆ é™¤
            is_network_related = result.get('is_network_related', True)
            if not is_network_related:
                self._handle_non_network_content(update_id, update_data)
                return {'deleted': True, 'reason': 'not_network_related'}
            
            # 3. è®°å½• product_subcategory ä¸ºç©ºçš„æƒ…å†µï¼ˆä¸åˆ é™¤ï¼Œä»…è®°å½•ï¼‰
            product_subcategory = result.get('product_subcategory', '')
            if not product_subcategory:
                self._record_quality_issue(
                    update_id=update_id,
                    issue_type='empty_subcategory',
                    auto_action='kept',
                    vendor=vendor,
                    title=title,
                    source_url=source_url
                )
                self.logger.warning(f"subcategoryä¸ºç©º: {title[:50]}...")
            
            # 4. ä¿å­˜åˆ°æ–‡ä»¶ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            should_save_file = save_to_file if save_to_file is not None else self.enable_file_save
            if should_save_file:
                file_path = self._save_analysis_to_file(update_id, update_data, result)
                if file_path:
                    result['analysis_filepath'] = file_path
                    self.logger.debug(f"åˆ†æç»“æœå·²ä¿å­˜è‡³æ–‡ä»¶: {file_path}")
            
            # 5. æ›´æ–°æ•°æ®åº“
            if save_to_db:
                success = self.data_layer.update_analysis_fields(update_id, result)
                if not success:
                    self.logger.error(f"æ•°æ®åº“æ›´æ–°å¤±è´¥: {update_id}")
                    return None
            
            return result
            
        except Exception as e:
            self.logger.error(f"åˆ†ææ‰§è¡Œå¼‚å¸¸ {update_id}: {e}")
            return None
    
    def _save_analysis_to_file(
        self, 
        update_id: str, 
        update_data: Dict[str, Any], 
        result: Dict[str, Any]
    ) -> Optional[str]:
        """
        ä¿å­˜åˆ†æç»“æœåˆ°æ–‡ä»¶
        
        Args:
            update_id: æ›´æ–°è®°å½•ID
            update_data: åŸå§‹æ›´æ–°æ•°æ®
            result: åˆ†æç»“æœ
            
        Returns:
            æ–‡ä»¶è·¯å¾„ï¼Œå¤±è´¥è¿”å› None
        """
        try:
            # åˆ›å»ºè¾“å‡ºç›®å½•
            vendor = update_data.get('vendor', 'unknown')
            output_dir = os.path.join(self.output_base_dir, vendor)
            os.makedirs(output_dir, exist_ok=True)
            
            # æ„å»ºåˆ†ææ•°æ®
            analysis_data = {
                'update_id': update_id,
                'vendor': vendor,
                'source_channel': update_data.get('source_channel', ''),
                'original_title': update_data.get('title', ''),
                'source_url': update_data.get('source_url', ''),
                'publish_date': update_data.get('publish_date', ''),
                'analyzed_at': datetime.now().isoformat(),
                'analysis': {
                    'title_translated': result.get('title_translated', ''),
                    'content_summary': result.get('content_summary', ''),
                    'update_type': result.get('update_type', ''),
                    'product_subcategory': result.get('product_subcategory', ''),
                    'tags': json.loads(result.get('tags', '[]')) if isinstance(result.get('tags'), str) else result.get('tags', [])
                }
            }
            
            # ç”Ÿæˆæ–‡ä»¶è·¯å¾„
            filename = f"{update_id}.json"
            file_path = os.path.join(output_dir, filename)
            
            # å†™å…¥æ–‡ä»¶
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(analysis_data, f, ensure_ascii=False, indent=2)
            
            return file_path
            
        except Exception as e:
            self.logger.error(f"ä¿å­˜åˆ†ææ–‡ä»¶å¤±è´¥: {e}")
            return None
    
    def _handle_non_network_content(self, update_id: str, update_data: Dict[str, Any]) -> None:
        """
        å¤„ç†éç½‘ç»œç›¸å…³å†…å®¹ï¼šåˆ é™¤è®°å½•å¹¶è®°å½•è´¨é‡é—®é¢˜
        
        Args:
            update_id: è®°å½•ID
            update_data: æ›´æ–°æ•°æ®
        """
        title = update_data.get('title', '')
        source_url = update_data.get('source_url', '')
        vendor = update_data.get('vendor', '')
        raw_filepath = update_data.get('raw_filepath', '')
        
        # 1. åˆ é™¤æ•°æ®åº“è®°å½•
        try:
            self.data_layer.delete_update(update_id)
        except Exception as e:
            self.logger.error(f"åˆ é™¤æ•°æ®åº“è®°å½•å¤±è´¥: {e}")
        
        # 2. åˆ é™¤åŸå§‹æ–‡ä»¶
        if raw_filepath and os.path.exists(raw_filepath):
            try:
                os.remove(raw_filepath)
                self.logger.debug(f"åˆ é™¤åŸå§‹æ–‡ä»¶: {raw_filepath}")
            except Exception as e:
                self.logger.error(f"åˆ é™¤åŸå§‹æ–‡ä»¶å¤±è´¥: {e}")
        
        # 3. è®°å½•è´¨é‡é—®é¢˜åˆ°æ•°æ®åº“
        self._record_quality_issue(
            update_id=update_id,
            issue_type='not_network_related',
            auto_action='deleted',
            vendor=vendor,
            title=title,
            source_url=source_url
        )
        
        self.logger.info(f"åˆ é™¤[éç½‘ç»œå†…å®¹]: {title[:50]}...")
    
    def _record_quality_issue(
        self,
        update_id: str,
        issue_type: str,
        auto_action: str,
        vendor: str = '',
        title: str = '',
        source_url: str = ''
    ) -> None:
        """
        è®°å½•è´¨é‡é—®é¢˜åˆ°æ•°æ®åº“
        
        Args:
            update_id: è®°å½•ID
            issue_type: é—®é¢˜ç±»å‹
            auto_action: è‡ªåŠ¨åŠ¨ä½œ (deleted/kept)
            vendor: å‚å•†
            title: æ ‡é¢˜
            source_url: æ¥æºé“¾æ¥
        """
        try:
            self.data_layer.insert_quality_issue(
                update_id=update_id,
                issue_type=issue_type,
                auto_action=auto_action,
                vendor=vendor,
                title=title,
                source_url=source_url,
                batch_id=AnalysisExecutor._current_batch_id
            )
        except Exception as e:
            self.logger.error(f"è®°å½•è´¨é‡é—®é¢˜å¤±è´¥: {e}")
    
    @classmethod
    def print_analysis_report(cls, data_layer) -> None:
        """
        è¾“å‡ºåˆ†ææŠ¥å‘Šï¼ˆä»æ•°æ®åº“è¯»å–ï¼‰
        
        Args:
            data_layer: UpdateDataLayer å®ä¾‹
        """
        try:
            stats = data_layer.get_issue_statistics()
            
            if stats['total_open'] == 0 and stats['total_resolved'] == 0:
                return
            
            print("\n" + "=" * 60)
            print("ğŸ” è´¨é‡é—®é¢˜æŠ¥å‘Š")
            print("=" * 60)
            
            # é—®é¢˜ç»Ÿè®¡è¡¨æ ¼
            status_data = [
                ["âš ï¸  å¾…å¤„ç†", stats['total_open']],
                ["âœ… å·²è§£å†³", stats['total_resolved']],
                ["â­ï¸  å·²å¿½ç•¥", stats['total_ignored']],
            ]
            print(tabulate(status_data, headers=["çŠ¶æ€", "æ•°é‡"], tablefmt="simple"))
            
            if stats['by_type']:
                print("\næŒ‰ç±»å‹ç»Ÿè®¡(å¾…å¤„ç†):")
                type_data = [[t, c] for t, c in stats['by_type'].items()]
                print(tabulate(type_data, headers=["é—®é¢˜ç±»å‹", "æ•°é‡"], tablefmt="simple"))
            
            if stats['total_open'] > 0:
                print("\nğŸ’¡ ä½¿ç”¨ ./run.sh check --issues æŸ¥çœ‹è¯¦æƒ…")
            
            print("=" * 60 + "\n")
            
        except Exception as e:
            logger.error(f"è·å–åˆ†ææŠ¥å‘Šå¤±è´¥: {e}")
