# ğŸ—ï¸ CloudNetSpy API æ¨¡å—æ¶æ„æ–¹æ¡ˆ

> **ç‰ˆæœ¬**: v2.0  
> **æ›´æ–°æ—¥æœŸ**: 2024-12-26  
> **çŠ¶æ€**: ç”Ÿäº§å°±ç»ªæ–¹æ¡ˆ

## ä¸€ã€ä¸šåŠ¡ç›®æ ‡ä¸è®¾è®¡åŸåˆ™

### 1.1 æ ¸å¿ƒä¸šåŠ¡å®šä½

**é¡¹ç›®ä½¿å‘½**: äº‘è®¡ç®—ç«äº‰æƒ…æŠ¥ç³»ç»Ÿ - å¤šäº‘æ›´æ–°èšåˆ + AIæ™ºèƒ½åˆ†æ + æƒ…æŠ¥æ¨é€

**APIå±‚çš„èŒè´£**:
- ğŸ¯ **ä¸»è¦ç”¨æˆ·**: å†…éƒ¨å‰ç«¯ç•Œé¢ï¼ˆæ•°æ®çœ‹æ¿ã€æ›´æ–°åˆ—è¡¨ã€è¯¦æƒ…é¡µï¼‰
- ğŸ”§ **æ¬¡è¦ç”¨æˆ·**: åå°å®šæ—¶ä»»åŠ¡ï¼ˆçˆ¬è™«è§¦å‘ã€æ‰¹é‡åˆ†æã€æŠ¥å‘Šç”Ÿæˆï¼‰
- ğŸš€ **æœªæ¥æ‰©å±•**: ç¬¬ä¸‰æ–¹é›†æˆï¼ˆWebhookã€å¼€æ”¾APIï¼‰

### 1.2 åˆ†é˜¶æ®µå®æ–½ç­–ç•¥

| é˜¶æ®µ | ä¸šåŠ¡ç›®æ ‡ | æ ¸å¿ƒæ¥å£ | å®æ–½æ—¶é—´ |
|------|---------|---------|----------|
| **é˜¶æ®µä¸€ï¼šMVP** | æ”¯æŒå‰ç«¯åŸºç¡€æŸ¥è¯¢ + AIåˆ†æè§¦å‘ | æ›´æ–°åˆ—è¡¨/è¯¦æƒ…ã€ç»Ÿè®¡çœ‹æ¿ã€åˆ†æè§¦å‘ | **å½“å‰å¼€å‘** |
| **é˜¶æ®µäºŒï¼šé«˜çº§åˆ†æ** | æ”¯æŒæœˆåº¦/å¹´åº¦æŠ¥å‘Šç”Ÿæˆ | æŠ¥å‘Šç”Ÿæˆ/æŸ¥è¯¢ã€å‚å•†å¯¹æ¯” | Phase 6ï¼ˆåç»­ï¼‰ |
| **é˜¶æ®µä¸‰ï¼šè®¢é˜…æ¨é€** | æ”¯æŒç”¨æˆ·è®¢é˜…ä¸æ¶ˆæ¯æ¨é€ | è®¢é˜…ç®¡ç†ã€æ¨é€é…ç½®ã€é€šçŸ¥å†å² | Phase 4ï¼ˆåç»­ï¼‰ |

### 1.3 è®¾è®¡åŸåˆ™

- âœ… **ä¸šåŠ¡é©±åŠ¨**: æ¯ä¸ªæ¥å£éƒ½å¯¹åº”æ˜ç¡®çš„ä¸šåŠ¡åœºæ™¯
- âœ… **ä»£ç ä¸€è‡´æ€§**: ä¸¥æ ¼å¯¹é½æ•°æ®åº“Schemaä¸ç°æœ‰çˆ¬è™«ä»£ç 
- âœ… **å‰åç«¯è§£è€¦**: APIè¿”å›çº¯JSONï¼Œå‰ç«¯è‡ªç”±é€‰å‹
- âœ… **å¯æµ‹è¯•æ€§**: Serviceå±‚ç‹¬ç«‹ï¼Œæ˜“äºMockæµ‹è¯•
- âœ… **å¯è¿ç»´æ€§**: å†…ç½®å¥åº·æ£€æŸ¥ã€é”™è¯¯æ—¥å¿—ã€æ€§èƒ½ç›‘æ§

---

## äºŒã€ç›®å½•ç»“æ„

```plaintext
src/api/
â”œâ”€â”€ __init__.py                 # å¯¼å‡º app
â”œâ”€â”€ app.py                      # FastAPI åº”ç”¨å…¥å£
â”œâ”€â”€ config.py                   # API é…ç½®ç±»
â”œâ”€â”€ dependencies.py             # ä¾èµ–æ³¨å…¥ï¼ˆæ•°æ®åº“ã€è®¤è¯ç­‰ï¼‰
â”‚
â”œâ”€â”€ routes/                     # è·¯ç”±æ¨¡å—
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ health.py              # å¥åº·æ£€æŸ¥
â”‚   â”œâ”€â”€ updates.py             # æ›´æ–°æ•°æ®æ¥å£ï¼ˆæ ¸å¿ƒï¼‰
â”‚   â”œâ”€â”€ analysis.py            # AI åˆ†ææ¥å£
â”‚   â”œâ”€â”€ stats.py               # ç»Ÿè®¡åˆ†ææ¥å£
â”‚   â””â”€â”€ vendors.py             # å‚å•†/äº§å“å…ƒæ•°æ®æ¥å£
â”‚
â”œâ”€â”€ schemas/                    # Pydantic æ•°æ®æ¨¡å‹
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ update.py              # æ›´æ–°ç›¸å…³æ¨¡å‹
â”‚   â”œâ”€â”€ analysis.py            # åˆ†æç›¸å…³æ¨¡å‹
â”‚   â”œâ”€â”€ common.py              # å…¬å…±æ¨¡å‹ï¼ˆåˆ†é¡µã€å“åº”åŒ…è£…ï¼‰
â”‚   â””â”€â”€ stats.py               # ç»Ÿè®¡æ¨¡å‹
â”‚
â”œâ”€â”€ services/                   # ä¸šåŠ¡é€»è¾‘å±‚ï¼ˆé‡è¦ï¼ï¼‰
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ update_service.py      # æ›´æ–°æ•°æ®æœåŠ¡
â”‚   â”œâ”€â”€ analysis_service.py    # åˆ†ææœåŠ¡
â”‚   â””â”€â”€ stats_service.py       # ç»Ÿè®¡æœåŠ¡
â”‚
â”œâ”€â”€ middleware/                 # ä¸­é—´ä»¶
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ cors.py                # è·¨åŸŸé…ç½®
â”‚   â””â”€â”€ error_handler.py       # å…¨å±€é”™è¯¯å¤„ç†
â”‚
â””â”€â”€ utils/                      # å·¥å…·å‡½æ•°
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ response.py            # ç»Ÿä¸€å“åº”æ ¼å¼åŒ–
    â””â”€â”€ validators.py          # è‡ªå®šä¹‰éªŒè¯å™¨
```

---

## ä¸‰ã€æ ¸å¿ƒAPIæ¥å£è®¾è®¡

### 3.1 å¥åº·æ£€æŸ¥

```plaintext
GET  /                          # API æ ¹è·¯å¾„ï¼Œè¿”å›ç‰ˆæœ¬ä¿¡æ¯
GET  /health                    # å¥åº·æ£€æŸ¥ï¼ˆæ•°æ®åº“è¿æ¥çŠ¶æ€ï¼‰
GET  /docs                      # Swagger UIï¼ˆFastAPI è‡ªåŠ¨ç”Ÿæˆï¼‰
```

### 3.2 æ›´æ–°æ•°æ®æ¥å£ï¼ˆroutes/updates.pyï¼‰

```plaintext
GET  /api/v1/updates                    # åˆ—è¡¨æŸ¥è¯¢ï¼ˆæ ¸å¿ƒï¼‰
     å‚æ•°ï¼š
     - vendor: str                      # å‚å•†è¿‡æ»¤ï¼ˆaws/azure/gcpç­‰ï¼‰
     - source_channel: str              # æ¥æºç±»å‹ï¼ˆblog/whatsnewï¼‰
     - update_type: str                 # æ›´æ–°ç±»å‹ï¼ˆnew_feature/enhancementç­‰ï¼‰
     - product_name: str                # äº§å“åç§°ï¼ˆæ¨¡ç³ŠåŒ¹é…ï¼‰
     - product_category: str            # äº§å“åˆ†ç±»ï¼ˆNetworking/Computeç­‰ï¼‰
     - date_from/date_to: str           # æ—¥æœŸèŒƒå›´ï¼ˆYYYY-MM-DDï¼‰
     - has_analysis: bool               # æ˜¯å¦å·²AIåˆ†æ
     - keyword: str                     # å…³é”®è¯æœç´¢ï¼ˆæ ‡é¢˜+å†…å®¹ï¼‰
     - tags: str                        # æ ‡ç­¾è¿‡æ»¤ï¼ˆé€—å·åˆ†éš”ï¼‰
     - sort_by: str                     # æ’åºå­—æ®µï¼ˆpublish_date/crawl_timeï¼‰
     - order: str                       # æ’åºæ–¹å‘ï¼ˆasc/descï¼‰
     - page: int = 1                    # é¡µç 
     - page_size: int = 20              # æ¯é¡µæ•°é‡ï¼ˆæœ€å¤§100ï¼‰
     è¿”å›ï¼šåˆ†é¡µåˆ—è¡¨ï¼ˆUpdateBriefå¯¹è±¡ï¼‰

GET  /api/v1/updates/{update_id}        # å•æ¡è¯¦æƒ…
     è¿”å›ï¼šUpdateDetailå¯¹è±¡ï¼ˆå«AIåˆ†æå­—æ®µï¼‰

GET  /api/v1/updates/{update_id}/raw    # è·å–åŸå§‹ Markdown å†…å®¹
     å“åº”ï¼štext/markdown
```

### 3.3 AI åˆ†ææ¥å£ï¼ˆroutes/analysis.pyï¼‰

```plaintext
POST /api/v1/analysis/single            # å•æ¡åˆ†æï¼ˆåŒæ­¥ï¼‰
     Body: {"update_id": "xxx"}
     è¿”å›ï¼š{
         "success": true,
         "data": {
             "title_translated": "...",
             "content_summary": "...",
             "update_type": "new_feature",
             "tags": [...]
         },
         "execution_time_ms": 2500
     }

POST /api/v1/analysis/batch             # æ‰¹é‡åˆ†æï¼ˆå¼‚æ­¥ä»»åŠ¡ï¼‰
     Body: {
         "vendor": "aws",               # å¯é€‰
         "limit": 100,                  # å¯é€‰
         "force": false                 # å¯é€‰
     }
     è¿”å›ï¼š{"task_id": "xxx", "status": "queued", "total": 123}

GET  /api/v1/analysis/tasks/{task_id}   # æŸ¥è¯¢æ‰¹é‡ä»»åŠ¡çŠ¶æ€
     è¿”å›ï¼š{
         "task_id": "xxx",
         "status": "running",
         "progress": {"completed": 50, "total": 100},
         "errors": [],
         "started_at": "2024-01-01T10:00:00Z"
     }

GET  /api/v1/analysis/tasks             # ä»»åŠ¡åˆ—è¡¨
     è¿”å›ï¼šæœ€è¿‘çš„æ‰¹é‡åˆ†æä»»åŠ¡ï¼ˆåˆ†é¡µï¼‰
```

### 3.4 ç»Ÿè®¡åˆ†ææ¥å£ï¼ˆroutes/stats.pyï¼‰

```plaintext
GET  /api/v1/stats/overview             # å…¨å±€æ¦‚è§ˆ
     è¿”å›ï¼š{
         "total_updates": 1234,
         "vendors": {
             "aws": {"total": 500, "analyzed": 450},
             "azure": {"total": 400, "analyzed": 380}
         },
         "update_types": {
             "new_feature": 300,
             "enhancement": 250
         },
         "last_crawl_time": "2024-01-01T10:00:00Z",
         "analysis_coverage": 0.85
     }

GET  /api/v1/stats/timeline             # æ—¶é—´çº¿ç»Ÿè®¡
     å‚æ•°ï¼š
     - granularity: str                 # day/week/month
     - date_from/date_to: str
     - vendor: str                      # å¯é€‰
     è¿”å›ï¼š[
         {
             "date": "2024-01-01",
             "count": 10,
             "vendors": {"aws": 5, "azure": 5}
         }
     ]

GET  /api/v1/stats/vendors              # æŒ‰å‚å•†ç»Ÿè®¡
     å‚æ•°ï¼šdate_from, date_to
     è¿”å›ï¼š[
         {"vendor": "aws", "count": 500, "analyzed": 450}
     ]
```

### 3.5 å…ƒæ•°æ®æ¥å£ï¼ˆroutes/vendors.pyï¼‰

```plaintext
GET  /api/v1/vendors                    # å‚å•†åˆ—è¡¨
     è¿”å›ï¼š[
         {
             "vendor": "aws",
             "name": "Amazon Web Services",
             "total_updates": 500,
             "source_channels": ["blog", "whatsnew"]
         }
     ]

GET  /api/v1/vendors/{vendor}/products  # å‚å•†çš„äº§å“åˆ—è¡¨
     è¿”å›ï¼š[
         {"product_name": "VPC", "category": "Networking", "count": 100}
     ]

GET  /api/v1/update-types               # æ›´æ–°ç±»å‹æšä¸¾
     è¿”å›ï¼š[
         {"value": "new_feature", "label": "æ–°åŠŸèƒ½å‘å¸ƒ", "description": "..."}
     ]
```

---

## å››ã€æ•°æ®æ¨¡å‹è®¾è®¡ï¼ˆPydantic Schemasï¼‰

### 4.1 é€šç”¨å“åº”æ¨¡å‹ï¼ˆschemas/common.pyï¼‰

```python
from pydantic import BaseModel, Field
from typing import Generic, TypeVar, Optional, List

T = TypeVar('T')

class ApiResponse(BaseModel, Generic[T]):
    """ç»Ÿä¸€ API å“åº”æ ¼å¼"""
    success: bool = True
    data: Optional[T] = None
    message: str = ""
    error: Optional[str] = None

class PaginationMeta(BaseModel):
    """åˆ†é¡µå…ƒæ•°æ®"""
    page: int = Field(..., ge=1)
    page_size: int = Field(..., ge=1, le=100)
    total: int = Field(..., ge=0)
    total_pages: int = Field(..., ge=0)

class PaginatedResponse(BaseModel, Generic[T]):
    """åˆ†é¡µå“åº”"""
    items: List[T]
    pagination: PaginationMeta
```

### 4.2 æ›´æ–°æ•°æ®æ¨¡å‹ï¼ˆschemas/update.pyï¼‰

```python
from datetime import date
from typing import Optional, List
from pydantic import BaseModel, Field, field_validator

class UpdateBrief(BaseModel):
    """æ›´æ–°åˆ—è¡¨é¡¹ï¼ˆç®€åŒ–ç‰ˆï¼‰- ç”¨äºåˆ—è¡¨å±•ç¤º"""
    update_id: str
    vendor: str
    source_channel: str                 # blog/whatsnewï¼ˆæ•°æ®åº“å­—æ®µåï¼‰
    title: str
    title_translated: Optional[str] = None
    description: Optional[str] = None
    publish_date: date                  # ä»æ•°æ®åº“TEXTè½¬æ¢è€Œæ¥
    product_name: Optional[str] = None
    product_category: Optional[str] = None
    update_type: Optional[str] = None
    tags: List[str] = []                # ä»æ•°æ®åº“JSONå­—ç¬¦ä¸²è§£æ
    has_analysis: bool                  # è®¡ç®—å­—æ®µ
    
    @field_validator('publish_date', mode='before')
    @classmethod
    def parse_publish_date(cls, v):
        """å…¼å®¹æ•°æ®åº“TEXTç±»å‹æ—¥æœŸ"""
        if isinstance(v, str):
            from datetime import datetime
            return datetime.strptime(v, '%Y-%m-%d').date()
        return v
    
    class Config:
        json_schema_extra = {
            "example": {
                "update_id": "aws_blog_20240101_abc123",
                "vendor": "aws",
                "source_channel": "blog",
                "title": "Announcing VPC Lattice...",
                "title_translated": "AWSå‘å¸ƒVPC LatticeæœåŠ¡ç½‘æ ¼",
                "description": "VPC Lattice is a new service...",
                "publish_date": "2024-01-01",
                "product_name": "VPC",
                "product_category": "Networking",
                "update_type": "new_feature",
                "tags": ["VPC", "æœåŠ¡ç½‘æ ¼", "IPv6"],
                "has_analysis": True
            }
        }

class UpdateDetail(UpdateBrief):
    """æ›´æ–°è¯¦æƒ…ï¼ˆå®Œæ•´ç‰ˆï¼‰- ç”¨äºè¯¦æƒ…é¡µå±•ç¤º"""
    content: str
    content_summary: Optional[str] = None
    product_subcategory: Optional[str] = None
    source_url: str
    crawl_time: str                     # ISO 8601æ ¼å¼
    raw_filepath: Optional[str] = None
    analysis_filepath: Optional[str] = None

class UpdateQueryParams(BaseModel):
    """æŸ¥è¯¢å‚æ•°éªŒè¯"""
    vendor: Optional[str] = None
    source_channel: Optional[str] = None
    update_type: Optional[str] = None
    product_name: Optional[str] = None
    product_category: Optional[str] = None
    date_from: Optional[str] = None
    date_to: Optional[str] = None
    has_analysis: Optional[bool] = None
    keyword: Optional[str] = None
    tags: Optional[str] = None
    sort_by: str = "publish_date"
    order: str = "desc"
    page: int = Field(1, ge=1)
    page_size: int = Field(20, ge=1, le=100)
```

### 4.3 åˆ†æç›¸å…³æ¨¡å‹ï¼ˆschemas/analysis.pyï¼‰

```python
from pydantic import BaseModel
from typing import List, Optional

class AnalysisResult(BaseModel):
    """AI åˆ†æç»“æœ"""
    title_translated: str
    content_summary: str
    update_type: str
    product_subcategory: str
    tags: List[str]

class AnalysisTaskStatus(BaseModel):
    """æ‰¹é‡åˆ†æä»»åŠ¡çŠ¶æ€"""
    task_id: str
    status: str                     # queued/running/completed/failed
    progress: dict                  # {"completed": 50, "total": 100}
    started_at: str
    estimated_completion: Optional[str] = None
    completed_at: Optional[str] = None
    errors: List[str] = []
```

---

## äº”ã€Service å±‚è®¾è®¡

### 5.1 UpdateServiceï¼ˆservices/update_service.pyï¼‰

```python
import json
from typing import List, Dict, Optional, Tuple
from datetime import datetime
from src.storage.database.sqlite_layer import UpdateDataLayer
from src.api.schemas.common import PaginationMeta

class UpdateService:
    """æ›´æ–°æ•°æ®ä¸šåŠ¡æœåŠ¡"""
    
    def __init__(self, db: UpdateDataLayer):
        self.db = db
    
    def get_updates_paginated(
        self, 
        filters: dict, 
        page: int, 
        page_size: int,
        sort_by: str = "publish_date",
        order: str = "desc"
    ) -> Tuple[List[Dict], PaginationMeta]:
        """åˆ†é¡µæŸ¥è¯¢æ›´æ–°åˆ—è¡¨"""
        # 1. æŸ¥è¯¢æ€»æ•°
        total = self.db.count_updates_with_filters(**filters)
        
        # 2. è®¡ç®—åˆ†é¡µ
        total_pages = (total + page_size - 1) // page_size
        offset = (page - 1) * page_size
        
        # 3. æŸ¥è¯¢å½“å‰é¡µæ•°æ®
        rows = self.db.query_updates_paginated(
            filters=filters, 
            limit=page_size, 
            offset=offset, 
            sort_by=sort_by, 
            order=order
        )
        
        # 4. å¤„ç†æ•°æ®
        items = [self._process_update_row(row) for row in rows]
        
        # 5. è¿”å›æ•°æ® + åˆ†é¡µå…ƒæ•°æ®
        pagination = PaginationMeta(
            page=page,
            page_size=page_size,
            total=total,
            total_pages=total_pages
        )
        
        return items, pagination
    
    def get_update_detail(self, update_id: str) -> Optional[Dict]:
        """è·å–æ›´æ–°è¯¦æƒ…"""
        row = self.db.get_update_by_id(update_id)
        if not row:
            return None
        
        return self._process_update_row(row)
    
    def _process_update_row(self, row: dict) -> dict:
        """
        å¤„ç†æ•°æ®åº“è¡Œï¼Œè½¬æ¢ä¸ºAPIæ ¼å¼
        
        å…³é”®å¤„ç†ï¼š
        1. tags: JSONå­—ç¬¦ä¸² -> Python list
        2. has_analysis: åŸºäº title_translated å­—æ®µå¢å¼ºåˆ¤å®š
        3. publish_date: TEXT -> dateå¯¹è±¡
        4. è¿‡æ»¤æ‰å‰ç«¯ä¸éœ€è¦çš„å­—æ®µ
        """
        result = dict(row)
        
        # 1. è§£ætags JSONå­—ç¬¦ä¸²
        tags_str = result.get('tags')
        if tags_str:
            try:
                result['tags'] = json.loads(tags_str)
                if not isinstance(result['tags'], list):
                    result['tags'] = []
            except (json.JSONDecodeError, TypeError):
                result['tags'] = []
        else:
            result['tags'] = []
        
        # 2. åˆ¤å®šæ˜¯å¦å·²åˆ†æï¼ˆå¢å¼ºéªŒè¯ï¼Œæ’é™¤æ— æ•ˆå€¼ï¼‰
        title_trans = result.get('title_translated', '').strip()
        result['has_analysis'] = bool(
            title_trans and 
            len(title_trans) >= 2 and  # æ’é™¤å•å­—ç¬¦æ— æ•ˆå€¼
            title_trans not in ['N/A', 'æš‚æ— ', 'None', 'null']  # æ’é™¤å¸¸è§æ— æ•ˆå€¼
        )
        
        # 3. è½¬æ¢æ—¥æœŸç±»å‹
        if 'publish_date' in result and isinstance(result['publish_date'], str):
            try:
                result['publish_date'] = datetime.strptime(result['publish_date'], '%Y-%m-%d').date()
            except ValueError:
                pass  # ä¿ç•™åŸå§‹å­—ç¬¦ä¸²
        
        # 4. è¿‡æ»¤æ‰å‰ç«¯ä¸éœ€è¦çš„å†…éƒ¨å­—æ®µ
        internal_fields = ['source_identifier', 'file_hash', 'metadata_json', 'priority']
        for field in internal_fields:
            result.pop(field, None)
        
        return result
```

### 5.2 AnalysisServiceï¼ˆservices/analysis_service.pyï¼‰

```python
import json
import uuid
from typing import Dict, Optional
from datetime import datetime
from src.storage.database.sqlite_layer import UpdateDataLayer
from src.analyzers.update_analyzer import UpdateAnalyzer
from src.utils.threading.thread_pool import AdaptiveThreadPool

class AnalysisService:
    """AI åˆ†æä¸šåŠ¡æœåŠ¡"""
    
    def __init__(self, db: UpdateDataLayer, analyzer: UpdateAnalyzer, config: Dict):
        self.db = db
        self.analyzer = analyzer
        
        # æ‰¹é‡å¤„ç†é…ç½®
        batch_config = config.get('ai_model', {}).get('batch_processing', {})
        self.max_workers = batch_config.get('max_workers', 10)
        api_rate_limit = config.get('ai_model', {}).get('api', {}).get('rate_limit', 60)
        
        # åˆå§‹åŒ–çº¿ç¨‹æ± 
        self.thread_pool = AdaptiveThreadPool(
            api_rate_limit=api_rate_limit,
            initial_threads=2,
            max_threads=self.max_workers
        )
    
    def analyze_single(self, update_id: str) -> Dict:
        """åŒæ­¥åˆ†æå•æ¡"""
        # 1. æŸ¥è¯¢æ›´æ–°æ•°æ®
        update = self.db.get_update_by_id(update_id)
        if not update:
            raise ValueError(f"æ›´æ–°è®°å½•ä¸å­˜åœ¨: {update_id}")
        
        # 2. è°ƒç”¨ UpdateAnalyzer.analyze()
        result = self.analyzer.analyze(update)
        
        if not result:
            raise RuntimeError(f"åˆ†æå¤±è´¥: {update_id}")
        
        # 3. æ³¨æ„ï¼šUpdateAnalyzerå·²åœ¨å†…éƒ¨å®Œæˆtagsåºåˆ—åŒ–ï¼Œæ— éœ€é‡å¤å¤„ç†
        
        # 4. ä¿å­˜åˆ†æç»“æœåˆ°æ–‡ä»¶ï¼ˆå¯é€‰ï¼‰
        file_path = self._save_analysis_to_file(update_id, update, result)
        if file_path:
            result['analysis_filepath'] = file_path
        
        # 5. æ›´æ–°æ•°æ®åº“
        success = self.db.update_analysis_fields(update_id, result)
        
        if not success:
            raise RuntimeError(f"æ›´æ–°åˆ†æç»“æœå¤±è´¥: {update_id}")
        
        return result
    
    def analyze_batch_async(
        self, 
        vendor: Optional[str], 
        limit: int,
        force: bool
    ) -> str:
        """å¼‚æ­¥æ‰¹é‡åˆ†æï¼ˆè¿”å› task_idï¼‰"""
        # 1. ç”Ÿæˆä»»åŠ¡ID
        task_id = str(uuid.uuid4())
        
        # 2. æŸ¥è¯¢å¾…åˆ†æè®°å½•
        updates = self.db.get_unanalyzed_updates(
            limit=limit,
            vendor=vendor,
            include_analyzed=force
        )
        
        if not updates:
            raise ValueError("æ²¡æœ‰å¾…åˆ†æçš„è®°å½•")
        
        # 3. åˆ›å»ºä»»åŠ¡è®°å½•
        self.db.create_analysis_task({
            'task_id': task_id,
            'task_name': 'batch_analysis',
            'task_status': 'queued',
            'vendor': vendor,
            'total_count': len(updates),
            'completed_count': 0,
            'started_at': datetime.now().isoformat()
        })
        
        # 4. å¯åŠ¨çº¿ç¨‹æ± å¼‚æ­¥å¤„ç†
        self.thread_pool.start()
        
        for update in updates:
            self.thread_pool.add_task(
                self._analyze_single_item,
                update,
                task_id,
                task_meta={'identifier': update['update_id']}
            )
        
        return task_id
    
    def get_task_status(self, task_id: str) -> Dict:
        """æŸ¥è¯¢ä»»åŠ¡çŠ¶æ€"""
        task = self.db.get_task_by_id(task_id)
        if not task:
            raise ValueError(f"ä»»åŠ¡ä¸å­˜åœ¨: {task_id}")
        
        return {
            'task_id': task['task_id'],
            'status': task['task_status'],
            'progress': {
                'completed': task.get('completed_count', 0),
                'total': task.get('total_count', 0)
            },
            'started_at': task['started_at'],
            'completed_at': task.get('completed_at'),
            'errors': json.loads(task.get('error_message', '[]'))
        }
    
    def _analyze_single_item(self, update_data: Dict, task_id: str) -> bool:
        """åˆ†æå•æ¡è®°å½•ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰"""
        update_id = update_data.get('update_id')
        
        try:
            # æ‰§è¡Œåˆ†æ
            result = self.analyzer.analyze(update_data)
            
            if result:
                # ä¿å­˜åˆ†æç»“æœåˆ°æ–‡ä»¶
                file_path = self._save_analysis_to_file(update_id, update_data, result)
                if file_path:
                    result['analysis_filepath'] = file_path
                
                # æ›´æ–°æ•°æ®åº“
                success = self.db.update_analysis_fields(update_id, result)
                
                # æ›´æ–°ä»»åŠ¡è¿›åº¦
                self.db.increment_task_progress(task_id, success)
                
                return success
            else:
                self.db.increment_task_progress(task_id, False)
                return False
                
        except Exception as e:
            self.db.increment_task_progress(task_id, False, str(e))
            return False
    
    def _save_analysis_to_file(self, update_id: str, update_data: Dict, result: Dict) -> Optional[str]:
        """ä¿å­˜åˆ†æç»“æœåˆ°æ–‡ä»¶"""
        # å‚è€ƒ scripts/analyze_updates.py å®ç°
        # è¿”å›æ–‡ä»¶è·¯å¾„æˆ–None
        pass
```

---

## å…­ã€æ•°æ®åº“å±‚æ‰©å±•æ–¹æ³•

### 6.1 é€šç”¨åˆ†é¡µæŸ¥è¯¢ï¼ˆæ·»åŠ åˆ° UpdateDataLayerï¼‰

**âš ï¸ é‡è¦æç¤º**: 
- ç°æœ‰`count_updates()`æ–¹æ³•ä»…æ”¯æŒ4ä¸ªè¿‡æ»¤æ¡ä»¶ï¼Œ**å¿…é¡»æ‰©å±•**
- ä»¥ä¸‹æ–¹æ³•å‡ä¸º**æ–°å¢æ–¹æ³•**ï¼Œéœ€æ·»åŠ åˆ°`sqlite_layer.py`
- å¿…é¡»ä¸¥æ ¼éªŒè¯è¾“å…¥å‚æ•°ï¼Œé˜²æ­¢SQLæ³¨å…¥

```python
def query_updates_paginated(
    self,
    filters: Dict[str, Any],
    limit: int,
    offset: int,
    sort_by: str = "publish_date",
    order: str = "desc"
) -> List[Dict[str, Any]]:
    """
    é€šç”¨åˆ†é¡µæŸ¥è¯¢æ–¹æ³•
    
    Args:
        filters: è¿‡æ»¤æ¡ä»¶å­—å…¸ï¼Œæ”¯æŒï¼š
            - vendor, source_channel, update_type
            - product_nameï¼ˆæ¨¡ç³ŠåŒ¹é…ï¼‰, product_category
            - date_from, date_to
            - has_analysis
            - keywordï¼ˆæœç´¢title+contentï¼‰
            - tagsï¼ˆé€—å·åˆ†éš”ï¼ŒORåŒ¹é…ï¼‰
        limit: æ¯é¡µæ•°é‡
        offset: åç§»é‡
        sort_by: æ’åºå­—æ®µ
        order: æ’åºæ–¹å‘
    """
    try:
        with self._get_connection() as conn:
            cursor = conn.cursor()
            
            where_clauses = []
            params = []
            
            # vendorè¿‡æ»¤
            if filters.get('vendor'):
                where_clauses.append("vendor = ?")
                params.append(filters['vendor'])
            
            # source_channelè¿‡æ»¤
            if filters.get('source_channel'):
                where_clauses.append("source_channel = ?")
                params.append(filters['source_channel'])
            
            # update_typeè¿‡æ»¤
            if filters.get('update_type'):
                where_clauses.append("update_type = ?")
                params.append(filters['update_type'])
            
            # product_nameæ¨¡ç³ŠåŒ¹é…
            if filters.get('product_name'):
                where_clauses.append("product_name LIKE ?")
                params.append(f"%{filters['product_name']}%")
            
            # product_categoryè¿‡æ»¤
            if filters.get('product_category'):
                where_clauses.append("product_category = ?")
                params.append(filters['product_category'])
            
            # æ—¥æœŸèŒƒå›´
            if filters.get('date_from'):
                where_clauses.append("publish_date >= ?")
                params.append(filters['date_from'])
            
            if filters.get('date_to'):
                where_clauses.append("publish_date <= ?")
                params.append(filters['date_to'])
            
            # has_analysisè¿‡æ»¤
            if filters.get('has_analysis') is not None:
                if filters['has_analysis']:
                    where_clauses.append("title_translated IS NOT NULL AND title_translated != ''")
                else:
                    where_clauses.append("(title_translated IS NULL OR title_translated = '')")
            
            # keywordå…³é”®è¯æœç´¢
            if filters.get('keyword'):
                where_clauses.append("(title LIKE ? OR content LIKE ?)")
                keyword_param = f"%{filters['keyword']}%"
                params.extend([keyword_param, keyword_param])
            
            # tagsæ ‡ç­¾è¿‡æ»¤
            # âš ï¸ æ€§èƒ½è­¦å‘Š: LIKEæŸ¥è¯¢æ— æ³•ä½¿ç”¨ç´¢å¼•ï¼Œå¤§æ•°æ®é‡æ—¶è€ƒè™‘ä½¿ç”¨FTS5å…¨æ–‡æœç´¢
            if filters.get('tags'):
                tag_list = [t.strip() for t in filters['tags'].split(',')]
                tag_conditions = []
                for tag in tag_list:
                    tag_conditions.append("tags LIKE ?")
                    # æ³¨æ„ï¼šåŒ¹é…JSONæ•°ç»„ä¸­çš„å­—ç¬¦ä¸²å€¼
                    params.append(f'%"{tag}"%')
                where_clauses.append(f"({' OR '.join(tag_conditions)})")
            
            # æ„å»ºWHEREå­å¥
            where_clause = " AND ".join(where_clauses) if where_clauses else "1=1"
            
            # éªŒè¯æ’åºå­—æ®µ
            allowed_sort_fields = ['publish_date', 'crawl_time', 'update_id', 'vendor']
            if sort_by not in allowed_sort_fields:
                sort_by = 'publish_date'
            
            # éªŒè¯æ’åºæ–¹å‘ï¼ˆé˜²æ­¢SQLæ³¨å…¥ï¼‰
            order = order.upper()
            if order not in ['ASC', 'DESC']:
                order = 'DESC'
            
            # æ„å»ºSQL
            sql = f"""
                SELECT * FROM updates
                WHERE {where_clause}
                ORDER BY {sort_by} {order}
                LIMIT ? OFFSET ?
            """
            params.extend([limit, offset])
            
            cursor.execute(sql, params)
            return [dict(row) for row in cursor.fetchall()]
            
    except Exception as e:
        self.logger.error(f"åˆ†é¡µæŸ¥è¯¢å¤±è´¥: {e}")
        return []
```

### 6.2 æ‰©å±•ç»Ÿè®¡æ–¹æ³•

```python
def count_updates_with_filters(self, **filters) -> int:
    """æ‰©å±•ç‰ˆç»Ÿè®¡æ–¹æ³•ï¼ˆæ”¯æŒæ‰€æœ‰è¿‡æ»¤æ¡ä»¶ï¼‰"""
    # å¤ç”¨ query_updates_paginated çš„è¿‡æ»¤é€»è¾‘ï¼Œæ”¹ä¸º COUNT(*)
    try:
        with self._get_connection() as conn:
            cursor = conn.cursor()
            
            # å¤ç”¨ç›¸åŒçš„è¿‡æ»¤é€»è¾‘
            where_clauses = []
            params = []
            
            # ... (ä¸ query_updates_paginated ç›¸åŒçš„è¿‡æ»¤é€»è¾‘)
            
            where_clause = " AND ".join(where_clauses) if where_clauses else "1=1"
            sql = f"SELECT COUNT(*) as count FROM updates WHERE {where_clause}"
            
            cursor.execute(sql, params)
            result = cursor.fetchone()
            return result['count'] if result else 0
            
    except Exception as e:
        self.logger.error(f"ç»Ÿè®¡æŸ¥è¯¢å¤±è´¥: {e}")
        return 0

def get_vendor_statistics(
    self, 
    date_from: Optional[str] = None, 
    date_to: Optional[str] = None
) -> List[Dict[str, Any]]:
    """æŒ‰å‚å•†ç»Ÿè®¡"""
    try:
        with self._get_connection() as conn:
            cursor = conn.cursor()
            
            where_clauses = []
            params = []
            
            if date_from:
                where_clauses.append("publish_date >= ?")
                params.append(date_from)
            
            if date_to:
                where_clauses.append("publish_date <= ?")
                params.append(date_to)
            
            where_clause = " AND ".join(where_clauses) if where_clauses else "1=1"
            
            sql = f"""
                SELECT 
                    vendor,
                    COUNT(*) as count,
                    SUM(CASE WHEN title_translated IS NOT NULL AND title_translated != '' 
                        THEN 1 ELSE 0 END) as analyzed
                FROM updates
                WHERE {where_clause}
                GROUP BY vendor
                ORDER BY count DESC
            """
            
            cursor.execute(sql, params)
            return [dict(row) for row in cursor.fetchall()]
            
    except Exception as e:
        self.logger.error(f"å‚å•†ç»Ÿè®¡æŸ¥è¯¢å¤±è´¥: {e}")
        return []

def get_analysis_coverage(self) -> float:
    """è®¡ç®—åˆ†æè¦†ç›–ç‡"""
    try:
        with self._get_connection() as conn:
            cursor = conn.cursor()
            
            cursor.execute("SELECT COUNT(*) as total FROM updates")
            total = cursor.fetchone()['total']
            
            if total == 0:
                return 0.0
            
            # âš ï¸ æ³¨æ„ï¼šå¢å¼ºhas_analysisåˆ¤å®šï¼Œæ’é™¤æ— æ•ˆå€¼
            cursor.execute(
                "SELECT COUNT(*) as analyzed FROM updates "
                "WHERE title_translated IS NOT NULL "
                "AND title_translated != '' "
                "AND LENGTH(TRIM(title_translated)) >= 2"  # æ’é™¤å•å­—ç¬¦æ— æ•ˆå€¼
            )
            analyzed = cursor.fetchone()['analyzed']
            
            return round(analyzed / total, 4)
            
    except Exception as e:
        self.logger.error(f"åˆ†æè¦†ç›–ç‡è®¡ç®—å¤±è´¥: {e}")
        return 0.0

# ==================== æ‰¹é‡åˆ†æä»»åŠ¡ç®¡ç†æ–¹æ³•ï¼ˆæ–°å¢ï¼‰ ====================

def create_analysis_task(self, task_data: Dict[str, Any]) -> bool:
    """åˆ›å»ºæ‰¹é‡åˆ†æä»»åŠ¡è®°å½•"""
    try:
        with self.lock:
            with self._get_connection() as conn:
                cursor = conn.cursor()
                
                cursor.execute('''
                    INSERT INTO analysis_tasks (
                        task_id, update_id, task_name, task_status,
                        task_result, started_at
                    ) VALUES (?, 'batch', ?, ?, ?, ?)
                ''', (
                    task_data['task_id'],
                    task_data.get('task_name', 'batch_analysis'),
                    task_data.get('task_status', 'queued'),
                    json.dumps({
                        'vendor': task_data.get('vendor'),
                        'total_count': task_data.get('total_count', 0),
                        'completed_count': 0
                    }),
                    task_data.get('started_at')
                ))
                
                conn.commit()
                return True
                
    except Exception as e:
        self.logger.error(f"åˆ›å»ºä»»åŠ¡å¤±è´¥: {e}")
        return False

def update_task_status(
    self, 
    task_id: str, 
    status: str, 
    progress: Optional[Dict] = None,
    error: Optional[str] = None
) -> bool:
    """æ›´æ–°ä»»åŠ¡çŠ¶æ€"""
    try:
        with self.lock:
            with self._get_connection() as conn:
                cursor = conn.cursor()
                
                update_fields = ['task_status = ?']
                params = [status]
                
                if progress:
                    update_fields.append('task_result = ?')
                    params.append(json.dumps(progress))
                
                if error:
                    update_fields.append('error_message = ?')
                    params.append(error)
                
                if status == 'completed' or status == 'failed':
                    update_fields.append('completed_at = ?')
                    params.append(datetime.now().isoformat())
                
                params.append(task_id)
                
                sql = f"UPDATE analysis_tasks SET {', '.join(update_fields)} WHERE task_id = ?"
                cursor.execute(sql, params)
                conn.commit()
                
                return cursor.rowcount > 0
                
    except Exception as e:
        self.logger.error(f"æ›´æ–°ä»»åŠ¡çŠ¶æ€å¤±è´¥: {e}")
        return False

def increment_task_progress(
    self, 
    task_id: str, 
    success: bool,
    error_msg: Optional[str] = None
) -> bool:
    """å¢åŠ ä»»åŠ¡è¿›åº¦è®¡æ•°ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰"""
    try:
        with self.lock:
            with self._get_connection() as conn:
                cursor = conn.cursor()
                
                # è·å–å½“å‰è¿›åº¦
                cursor.execute(
                    'SELECT task_result FROM analysis_tasks WHERE task_id = ?',
                    (task_id,)
                )
                row = cursor.fetchone()
                if not row:
                    return False
                
                result = json.loads(row['task_result'])
                result['completed_count'] = result.get('completed_count', 0) + 1
                
                if success:
                    result['success_count'] = result.get('success_count', 0) + 1
                else:
                    result['fail_count'] = result.get('fail_count', 0) + 1
                    if error_msg:
                        errors = result.get('errors', [])
                        errors.append(error_msg)
                        result['errors'] = errors[-100:]  # ä¿ç•™æœ€è¿‘100æ¡é”™è¯¯
                
                # åˆ¤æ–­æ˜¯å¦å®Œæˆ
                if result['completed_count'] >= result['total_count']:
                    status = 'completed'
                    completed_at = datetime.now().isoformat()
                    cursor.execute(
                        'UPDATE analysis_tasks SET task_status = ?, task_result = ?, completed_at = ? WHERE task_id = ?',
                        (status, json.dumps(result), completed_at, task_id)
                    )
                else:
                    cursor.execute(
                        'UPDATE analysis_tasks SET task_result = ? WHERE task_id = ?',
                        (json.dumps(result), task_id)
                    )
                
                conn.commit()
                return True
                
    except Exception as e:
        self.logger.error(f"æ›´æ–°ä»»åŠ¡è¿›åº¦å¤±è´¥: {e}")
        return False

def get_task_by_id(self, task_id: str) -> Optional[Dict[str, Any]]:
    """æ ¹æ®task_idè·å–ä»»åŠ¡è®°å½•"""
    try:
        with self._get_connection() as conn:
            cursor = conn.cursor()
            cursor.execute('SELECT * FROM analysis_tasks WHERE task_id = ?', (task_id,))
            
            row = cursor.fetchone()
            if row:
                task = dict(row)
                # è§£ætask_result JSON
                if task.get('task_result'):
                    task['task_result'] = json.loads(task['task_result'])
                return task
            return None
            
    except Exception as e:
        self.logger.error(f"è·å–ä»»åŠ¡è®°å½•å¤±è´¥: {e}")
        return None

def list_tasks_paginated(
    self, 
    limit: int = 20, 
    offset: int = 0
) -> List[Dict[str, Any]]:
    """åˆ†é¡µæŸ¥è¯¢ä»»åŠ¡åˆ—è¡¨ï¼ˆæŒ‰åˆ›å»ºæ—¶é—´å€’åºï¼‰"""
    try:
        with self._get_connection() as conn:
            cursor = conn.cursor()
            
            cursor.execute('''
                SELECT * FROM analysis_tasks
                WHERE task_name = 'batch_analysis'
                ORDER BY created_at DESC
                LIMIT ? OFFSET ?
            ''', (limit, offset))
            
            tasks = []
            for row in cursor.fetchall():
                task = dict(row)
                if task.get('task_result'):
                    task['task_result'] = json.loads(task['task_result'])
                tasks.append(task)
            
            return tasks
            
    except Exception as e:
        self.logger.error(f"æŸ¥è¯¢ä»»åŠ¡åˆ—è¡¨å¤±è´¥: {e}")
        return []
```

---

## ä¸ƒã€ä¸­é—´ä»¶ä¸å®‰å…¨

### 7.1 CORS é…ç½®ï¼ˆmiddleware/cors.pyï¼‰

```python
from fastapi.middleware.cors import CORSMiddleware

def setup_cors(app):
    """é…ç½®CORSä¸­é—´ä»¶"""
    app.add_middleware(
        CORSMiddleware,
        allow_origins=["http://localhost:3000"],  # å‰ç«¯å¼€å‘åœ°å€
        allow_credentials=True,
        allow_methods=["*"],
        allow_headers=["*"],
    )
```

### 7.2 å…¨å±€é”™è¯¯å¤„ç†ï¼ˆmiddleware/error_handler.pyï¼‰

```python
from fastapi import Request, status
from fastapi.responses import JSONResponse

async def global_exception_handler(request: Request, exc: Exception):
    """å…¨å±€å¼‚å¸¸å¤„ç†å™¨"""
    return JSONResponse(
        status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
        content={
            "success": False,
            "error": str(exc),
            "message": "æœåŠ¡å™¨å†…éƒ¨é”™è¯¯"
        }
    )
```

---

## å…«ã€æŠ€æœ¯æ ˆä¸ä¾èµ–

### 8.1 æ–°å¢ä¾èµ–ï¼ˆrequirements.txtï¼‰

```
# API æ¡†æ¶
fastapi==0.115.0
uvicorn[standard]==0.32.0
pydantic==2.9.0
pydantic-settings==2.6.0

# é™æµï¼ˆå¯é€‰ï¼‰
slowapi==0.1.9
```

### 8.2 é…ç½®ç®¡ç†ï¼ˆapi/config.pyï¼‰

```python
from pydantic_settings import BaseSettings
from typing import List

class APISettings(BaseSettings):
    """API é…ç½®"""
    app_name: str = "CloudNetSpy API"
    version: str = "2.0.0"
    debug: bool = False
    
    # æ•°æ®åº“
    db_path: str = "data/sqlite/updates.db"
    
    # CORS
    cors_origins: List[str] = ["http://localhost:3000"]
    
    class Config:
        env_file = ".env"
        env_prefix = "API_"

settings = APISettings()
```

---

## ä¹ã€éƒ¨ç½²ä¸è¿è¡Œ

### 9.1 å¯åŠ¨è„šæœ¬ï¼ˆæ·»åŠ åˆ° run.shï¼‰

```bash
# API æœåŠ¡å™¨å¯åŠ¨
api_server() {
    echo "å¯åŠ¨ API æœåŠ¡å™¨..."
    cd "${PROJECT_ROOT}"
    uvicorn src.api.app:app \
        --host 0.0.0.0 \
        --port 8000 \
        --reload \
        --log-level info
}

# ç”Ÿäº§æ¨¡å¼
api_server_prod() {
    uvicorn src.api.app:app \
        --host 0.0.0.0 \
        --port 8000 \
        --workers 4 \
        --log-level warning
}
```

---

## åã€å®æ–½æ£€æŸ¥æ¸…å•

### é˜¶æ®µ0ï¼šæ•°æ®åº“å±‚æ‰©å±•ï¼ˆå¿…é¡»å…ˆå®Œæˆï¼ŒP0ä¼˜å…ˆçº§ï¼‰

**â— å…³é”®ä¾èµ–**: ä»¥ä¸‹æ–¹æ³•æ˜¯APIå¼€å‘çš„åŸºç¡€ï¼Œå¿…é¡»ä¼˜å…ˆå®Œæˆ

#### 0.1 æ ¸å¿ƒæŸ¥è¯¢æ–¹æ³•ï¼ˆ1å¤©ï¼‰
- [ ] å®ç° `query_updates_paginated()`
  - æ”¯æŒ11ä¸ªè¿‡æ»¤æ¡ä»¶ï¼ˆvendor/source_channel/update_type/product_name/product_category/date_from/date_to/has_analysis/keyword/tags/sort_byï¼‰
  - ä¸¥æ ¼éªŒè¯sort_byå’Œorderå‚æ•°ï¼ˆé˜²SQLæ³¨å…¥ï¼‰
  - æ³¨æ„tagsè¿‡æ»¤ä½¿ç”¨LIKEæŸ¥è¯¢ï¼Œå¤§æ•°æ®é‡æ—¶éœ€ä¼˜åŒ–
- [ ] å®ç° `count_updates_with_filters()`
  - å¤ç”¨query_updates_paginatedçš„è¿‡æ»¤é€»è¾‘
  - å°†SELECT *æ”¹ä¸ºSELECT COUNT(*)
- [ ] å®ç° `get_vendor_statistics()`
  - æŒ‰å‚å•†ç»Ÿè®¡æ€»æ•°å’Œå·²åˆ†ææ•°
  - æ”¯æŒdate_from/date_toè¿‡æ»¤
- [ ] å®ç° `get_analysis_coverage()`
  - å¢å¼ºhas_analysisåˆ¤å®šï¼šLENGTH(TRIM(title_translated)) >= 2
  - è¿”å›åˆ†æè¦†ç›–ç‡ï¼ˆå°æ•°ï¼Œ4ä½ç²¾åº¦ï¼‰

#### 0.2 æ‰¹é‡ä»»åŠ¡ç®¡ç†æ–¹æ³•ï¼ˆ1å¤©ï¼‰
- [ ] å®ç° `create_analysis_task()`
  - æ’å…¥analysis_tasksè¡¨
  - task_resultå­—æ®µå­˜å‚¨JSONï¼š{vendor, total_count, completed_count, success_count, fail_count}
- [ ] å®ç° `update_task_status()`
  - æ›´æ–°task_status/task_result/completed_at
- [ ] å®ç° `increment_task_progress()`
  - çº¿ç¨‹å®‰å…¨çš„è¿›åº¦è®¡æ•°ï¼ˆä½¿ç”¨self.lockï¼‰
  - è‡ªåŠ¨åˆ¤æ–­ä»»åŠ¡å®ŒæˆçŠ¶æ€
- [ ] å®ç° `get_task_by_id()`
  - è¿”å›ä»»åŠ¡è¯¦æƒ…ï¼Œè‡ªåŠ¨è§£ætask_result JSON
- [ ] å®ç° `list_tasks_paginated()`
  - åˆ†é¡µæŸ¥è¯¢batch_analysisä»»åŠ¡
  - æŒ‰created_atå€’åº

#### 0.3 å•å…ƒæµ‹è¯•ï¼ˆ0.5å¤©ï¼‰
- [ ] æµ‹è¯•query_updates_paginatedè¿‡æ»¤é€»è¾‘
  - å•æ¡ä»¶è¿‡æ»¤
  - ç»„åˆæ¡ä»¶è¿‡æ»¤
  - keywordæœç´¢
  - tagsè¿‡æ»¤
  - æ’åºåŠŸèƒ½
- [ ] æµ‹è¯•æ‰¹é‡ä»»åŠ¡CRUD
  - å¹¶å‘åœºæ™¯ä¸‹çš„increment_task_progress
  - ä»»åŠ¡çŠ¶æ€æµè½¬æ­£ç¡®æ€§

### é˜¶æ®µ1ï¼šåŸºç¡€æ¡†æ¶ï¼ˆ1å¤©ï¼‰
- [ ] åˆ›å»ºç›®å½•ç»“æ„ï¼ˆå‚ç…§ç¬¬äºŒèŠ‚ï¼‰
- [ ] å®ç° app.pyï¼ˆFastAPI åˆå§‹åŒ–ï¼‰
  - é…ç½®CORSä¸­é—´ä»¶
  - æ³¨å†Œå…¨å±€å¼‚å¸¸å¤„ç†
  - æŒ‚è½½è·¯ç”±æ¨¡å—
- [ ] å®ç° dependencies.pyï¼ˆæ•°æ®åº“ä¾èµ–æ³¨å…¥ï¼‰
  - åˆ›å»ºget_db()ä¾èµ–å‡½æ•°
  - æ”¯æŒæµ‹è¯•æ—¶æ³¨å…¥Mock DB
- [ ] å®ç° schemas/common.pyï¼ˆé€šç”¨å“åº”æ¨¡å‹ï¼‰
  - ApiResponse[T]
  - PaginationMeta
  - PaginatedResponse[T]
- [ ] å®ç° routes/health.pyï¼ˆå¥åº·æ£€æŸ¥ï¼‰
  - GET / è¿”å›ç‰ˆæœ¬ä¿¡æ¯
  - GET /health æ£€æŸ¥æ•°æ®åº“è¿æ¥
- [ ] æµ‹è¯•ï¼š`curl http://localhost:8000/health`

### é˜¶æ®µ2ï¼šæ ¸å¿ƒæ¥å£ï¼ˆ3-4å¤©ï¼‰
- [ ] å®ç° services/update_service.py
- [ ] å®ç° schemas/update.py
- [ ] å®ç° routes/updates.pyï¼ˆåˆ—è¡¨ã€è¯¦æƒ…ã€rawï¼‰
- [ ] æµ‹è¯•ï¼šåˆ†é¡µåŠŸèƒ½
- [ ] æµ‹è¯•ï¼šè¿‡æ»¤åŠŸèƒ½ï¼ˆå•æ¡ä»¶ã€ç»„åˆæ¡ä»¶ã€keywordã€tagsï¼‰
- [ ] æµ‹è¯•ï¼šæ’åºåŠŸèƒ½

### é˜¶æ®µ3ï¼šåˆ†ææ¥å£ï¼ˆ2å¤©ï¼‰
- [ ] å®ç° services/analysis_service.py
  - â—æ³¨æ„ï¼šUpdateAnalyzerå·²å®Œæˆtagsåºåˆ—åŒ–ï¼ŒServiceå±‚æ— éœ€é‡å¤
  - å®ç°analyze_batch_async()è°ƒç”¨çº¿ç¨‹æ± 
  - ä»»åŠ¡çŠ¶æ€æŒä¹…åŒ–åˆ°analysis_tasksè¡¨
- [ ] å®ç° schemas/analysis.py
  - AnalysisResult
  - AnalysisTaskStatus
  - BatchAnalysisRequest
- [ ] å®ç° routes/analysis.pyï¼ˆå•æ¡ã€æ‰¹é‡ï¼‰
  - POST /api/v1/analysis/single
  - POST /api/v1/analysis/batch
  - GET /api/v1/analysis/tasks/{task_id}
  - GET /api/v1/analysis/tasks
- [ ] éªŒè¯ï¼štagså­—æ®µåœ¨Analyzerå·²åºåˆ—åŒ–ä¸ºJSONå­—ç¬¦ä¸²
- [ ] æµ‹è¯•ï¼šåˆ†ææµç¨‹
  - å•æ¡åŒæ­¥åˆ†æ
  - æ‰¹é‡å¼‚æ­¥åˆ†æä»»åŠ¡åˆ›å»º
  - ä»»åŠ¡çŠ¶æ€æŸ¥è¯¢

### é˜¶æ®µ4ï¼šç»Ÿè®¡æ¥å£ï¼ˆ2å¤©ï¼‰
- [ ] å®ç° services/stats_service.py
- [ ] å®ç° routes/stats.py
- [ ] æµ‹è¯•ï¼šå„ç±»ç»Ÿè®¡æŸ¥è¯¢

### é˜¶æ®µ5ï¼šå®‰å…¨ä¸ä¼˜åŒ–ï¼ˆ1-2å¤©ï¼‰
- [ ] é…ç½® CORS
- [ ] å…¨å±€é”™è¯¯å¤„ç†
- [ ] æ€§èƒ½æµ‹è¯•

### é˜¶æ®µ6ï¼šæ–‡æ¡£ä¸éƒ¨ç½²ï¼ˆ1å¤©ï¼‰
- [ ] å®Œå–„ OpenAPI æ–‡æ¡£
- [ ] ç¼–å†™éƒ¨ç½²è„šæœ¬
- [ ] ç¼–å†™å‰ç«¯å¯¹æ¥æ–‡æ¡£

---

## åä¸€ã€å­—æ®µå¯¹ç…§è¡¨

| æ•°æ®åº“å­—æ®µ | APIè¿”å›ç±»å‹ | æ•°æ®è½¬æ¢ | è¯´æ˜ |
|-----------|------------|---------|------|
| `update_id` | str | æ—  | ä¸»é”® |
| `vendor` | str | æ—  | å‚å•†æ ‡è¯† |
| `source_channel` | str | æ—  | blog/whatsnew |
| `update_type` | Optional[str] | æ—  | AIåˆ†ç±»ç»“æœ |
| `source_url` | str | æ—  | åŸå§‹URL |
| `title` | str | æ—  | è‹±æ–‡æ ‡é¢˜ |
| `title_translated` | Optional[str] | æ—  | ä¸­æ–‡æ ‡é¢˜ |
| `description` | Optional[str] | æ—  | ç®€è¦æè¿° |
| `content` | str | æ—  | Markdownå†…å®¹ |
| `content_summary` | Optional[str] | æ—  | AIæ‘˜è¦ |
| `publish_date` | TEXT | â†’ date | Serviceå±‚è½¬æ¢ |
| `crawl_time` | TEXT | â†’ str | ISO 8601æ ¼å¼ |
| `product_name` | Optional[str] | æ—  | äº§å“åç§° |
| `product_category` | Optional[str] | æ—  | äº§å“å¤§ç±» |
| `product_subcategory` | Optional[str] | æ—  | äº§å“å­ç±» |
| `tags` | TEXT(JSON) | â†’ List[str] | json.loads/dumps |
| `raw_filepath` | Optional[str] | æ—  | åŸå§‹æ–‡ä»¶è·¯å¾„ |
| `analysis_filepath` | Optional[str] | æ—  | åˆ†ææ–‡ä»¶è·¯å¾„ |
| `has_analysis` | è®¡ç®—å­—æ®µ | title_translatedåˆ¤å®š | ä¸å­˜å‚¨ |

**å†…éƒ¨å­—æ®µï¼ˆä¸è¿”å›ç»™å‰ç«¯ï¼‰**:
- `source_identifier`, `file_hash`, `metadata_json`, `priority`
- `created_at`, `updated_at`

---

## åäºŒã€å…³é”®å®ç°æ³¨æ„äº‹é¡¹

### 12.1 æ•°æ®ä¸€è‡´æ€§ä¿éšœ

#### has_analysisåˆ¤å®šé€»è¾‘ç»Ÿä¸€
```python
# âœ… æ­£ç¡®çš„åˆ¤å®šé€»è¾‘ï¼ˆä¸‰ä¸ªä½ç½®ä¿æŒä¸€è‡´ï¼‰ï¼š
# 1. UpdateDataLayer.get_unanalyzed_updates()
# 2. UpdateDataLayer.get_analysis_coverage()
# 3. UpdateService._process_update_row()

title_trans = result.get('title_translated', '').strip()
has_analysis = bool(
    title_trans and 
    len(title_trans) >= 2 and
    title_trans not in ['N/A', 'æš‚æ— ', 'None', 'null']
)
```

#### tagså­—æ®µåºåˆ—åŒ–è§„èŒƒ
```python
# âœ… åºåˆ—åŒ–åªåœ¨ä¸€ä¸ªåœ°æ–¹å®Œæˆï¼š
# UpdateAnalyzer._validate_and_fix_fields() å·²å®Œæˆåºåˆ—åŒ–
validated['tags'] = json.dumps(tags, ensure_ascii=False)

# âŒ Serviceå±‚ä¸è¦é‡å¤åºåˆ—åŒ–
# AnalysisService.analyze_single() ä¸­åˆ é™¤ä»¥ä¸‹ä»£ç ï¼š
# if 'tags' in result and isinstance(result['tags'], list):
#     result['tags'] = json.dumps(result['tags'], ensure_ascii=False)
```

### 12.2 å®‰å…¨æ€§è¦æ±‚

#### SQLæ³¨å…¥é˜²æŠ¤
```python
# âœ… å¿…é¡»ä½¿ç”¨ç™½åå•éªŒè¯
allowed_sort_fields = ['publish_date', 'crawl_time', 'update_id', 'vendor']
if sort_by not in allowed_sort_fields:
    sort_by = 'publish_date'

order = order.upper()
if order not in ['ASC', 'DESC']:
    order = 'DESC'

# âŒ ç¦æ­¢ç›´æ¥æ‹¼æ¥ç”¨æˆ·è¾“å…¥
# sql = f"SELECT * FROM updates ORDER BY {user_input}"  # å±é™©ï¼
```

#### å‚æ•°åŒ–æŸ¥è¯¢
```python
# âœ… æ‰€æœ‰è¿‡æ»¤æ¡ä»¶å¿…é¡»ä½¿ç”¨å‚æ•°åŒ–æŸ¥è¯¢
where_clauses.append("vendor = ?")
params.append(filters['vendor'])

# âŒ ç¦æ­¢å­—ç¬¦ä¸²æ‹¼æ¥
# sql = f"WHERE vendor = '{filters['vendor']}'"  # å±é™©ï¼
```

### 12.3 æ€§èƒ½ä¼˜åŒ–å»ºè®®

#### tagsè¿‡æ»¤æ€§èƒ½è­¦å‘Š
```python
# âš ï¸ å½“å‰å®ç°ï¼šLIKEæŸ¥è¯¢æ— æ³•ä½¿ç”¨ç´¢å¼•
tag_conditions.append("tags LIKE ?")
params.append(f'%"{tag}"%')

# ğŸ’¡ ä¼˜åŒ–æ–¹æ¡ˆï¼ˆåç»­é˜¶æ®µï¼‰ï¼š
# 1. SQLite 3.38+ ä½¿ç”¨ JSON_EXTRACT()
# 2. ä½¿ç”¨FTS5å…¨æ–‡æœç´¢
# 3. å»ºç«‹tagsåå‘ç´¢å¼•è¡¨
```

#### å¤åˆç´¢å¼•å»ºè®®
```sql
-- ä¼˜åŒ–has_analysisè¿‡æ»¤æŸ¥è¯¢
CREATE INDEX IF NOT EXISTS idx_updates_has_analysis 
ON updates(title_translated, publish_date)
WHERE title_translated IS NOT NULL AND title_translated != '';

-- ä¼˜åŒ–å‚å•†+æ—¥æœŸæŸ¥è¯¢ï¼ˆå·²å­˜åœ¨ï¼‰
-- CREATE INDEX idx_updates_vendor_date ON updates(vendor, publish_date);
```

### 12.4 æµ‹è¯•è¦æ±‚

#### æ•°æ®åº“æµ‹è¯•éš”ç¦»
```python
# âœ… ä½¿ç”¨å†…å­˜æ•°æ®åº“æµ‹è¯•
def test_query_updates():
    db = UpdateDataLayer(db_path=":memory:")
    # æµ‹è¯•é€»è¾‘

# âœ… æˆ–ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶
import tempfile
with tempfile.NamedTemporaryFile(suffix='.db') as f:
    db = UpdateDataLayer(db_path=f.name)
```

#### APIé›†æˆæµ‹è¯•
```python
from fastapi.testclient import TestClient

@pytest.fixture
def client():
    return TestClient(app)

def test_list_updates(client):
    response = client.get("/api/v1/updates?vendor=aws&page=1")
    assert response.status_code == 200
    data = response.json()
    assert data['success'] is True
    assert 'pagination' in data['data']
```

### 12.5 é”™è¯¯å¤„ç†è§„èŒƒ

#### Serviceå±‚å¼‚å¸¸å¤„ç†
```python
# âœ… æ˜ç¡®çš„å¼‚å¸¸ç±»å‹
if not update:
    raise ValueError(f"æ›´æ–°è®°å½•ä¸å­˜åœ¨: {update_id}")

if not success:
    raise RuntimeError(f"æ›´æ–°åˆ†æç»“æœå¤±è´¥: {update_id}")

# âœ… åœ¨Routeå±‚æ•è·å¹¶è½¬æ¢ä¸ºHTTPå“åº”
@router.post("/analysis/single")
async def analyze_single(request: AnalysisRequest):
    try:
        result = analysis_service.analyze_single(request.update_id)
        return ApiResponse(success=True, data=result)
    except ValueError as e:
        return ApiResponse(success=False, error=str(e), message="è®°å½•ä¸å­˜åœ¨")
    except RuntimeError as e:
        return ApiResponse(success=False, error=str(e), message="åˆ†æå¤±è´¥")
```

### 12.6 å·²çŸ¥é™åˆ¶ä¸æœªæ¥ä¼˜åŒ–

#### æ‰¹é‡ä»»åŠ¡é™åˆ¶
- å½“å‰è®¾è®¡ï¼šä»»åŠ¡çŠ¶æ€å­˜å‚¨åœ¨æ•°æ®åº“ï¼ŒæœåŠ¡é‡å¯åå¯æ¢å¤
- é™åˆ¶ï¼šæ— æ³•å®æ—¶æ¨é€è¿›åº¦ï¼ˆéœ€WebSocketæˆ–SSEï¼‰
- ä¼˜åŒ–æ–¹å‘ï¼šå¼•å…¥Rediså­˜å‚¨å®æ—¶è¿›åº¦+WebSocketæ¨é€

#### åˆ†é¡µæ€§èƒ½
- å½“å‰è®¾è®¡ï¼šä½¿ç”¨LIMIT/OFFSETåˆ†é¡µ
- é™åˆ¶ï¼šæ·±åˆ†é¡µæ€§èƒ½ä¸‹é™ï¼ˆOFFSET 10000æ€§èƒ½å·®ï¼‰
- ä¼˜åŒ–æ–¹å‘ï¼šåŸºäºæ¸¸æ ‡çš„åˆ†é¡µï¼ˆWHERE id > last_idï¼‰

#### tagsæœç´¢
- å½“å‰è®¾è®¡ï¼šLIKEåŒ¹é…JSONå­—ç¬¦ä¸²
- é™åˆ¶ï¼šæ— æ³•ä½¿ç”¨ç´¢å¼•ï¼Œæ€§èƒ½è¾ƒå·®
- ä¼˜åŒ–æ–¹å‘ï¼šFTS5å…¨æ–‡æœç´¢æˆ–tagsåå‘ç´¢å¼•è¡¨

---

## åä¸‰ã€å¼€å‘æ£€æŸ¥æ¸…å•é€ŸæŸ¥

### âœ… å¼€å‘å‰æ£€æŸ¥
- [ ] å·²é˜…è¯»ç¬¬åäºŒèŠ‚ã€Œå…³é”®å®ç°æ³¨æ„äº‹é¡¹ã€
- [ ] å·²ç†è§£has_analysisåˆ¤å®šé€»è¾‘
- [ ] å·²ç†è§£tagsåºåˆ—åŒ–è§„èŒƒï¼ˆä»…åœ¨Analyzerå®Œæˆï¼‰
- [ ] å·²å‡†å¤‡æµ‹è¯•æ•°æ®åº“ï¼ˆ:memory:æˆ–ä¸´æ—¶æ–‡ä»¶ï¼‰

### âœ… ä»£ç å®¡æŸ¥æ£€æŸ¥
- [ ] æ— SQLæ³¨å…¥é£é™©ï¼ˆå‚æ•°åŒ–æŸ¥è¯¢+ç™½åå•éªŒè¯ï¼‰
- [ ] has_analysisåˆ¤å®šé€»è¾‘ä¸€è‡´ï¼ˆä¸‰ä¸ªä½ç½®ï¼‰
- [ ] tagsæœªé‡å¤åºåˆ—åŒ–
- [ ] å¼‚å¸¸å¤„ç†æ˜ç¡®ï¼ˆValueError/RuntimeErrorï¼‰
- [ ] æµ‹è¯•è¦†ç›–ç‡>80%

### âœ… éƒ¨ç½²å‰æ£€æŸ¥
- [ ] æ•°æ®åº“ç´¢å¼•å·²åˆ›å»º
- [ ] APIæ–‡æ¡£å·²ç”Ÿæˆï¼ˆ/docsï¼‰
- [ ] å¥åº·æ£€æŸ¥æ­£å¸¸ï¼ˆ/healthï¼‰
- [ ] CORSé…ç½®æ­£ç¡®
- [ ] æ—¥å¿—çº§åˆ«è®¾ç½®ä¸ºWARNINGï¼ˆç”Ÿäº§ç¯å¢ƒï¼‰
