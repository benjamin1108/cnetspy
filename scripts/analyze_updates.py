#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
æ›´æ–°è®°å½•åˆ†æè„šæœ¬

å‘½ä»¤è¡Œå·¥å…·ï¼Œç”¨äºå¯¹çˆ¬å–çš„æ›´æ–°è®°å½•è¿›è¡Œ AI åˆ†æ
"""

import os
import sys
import argparse
import logging
import time
import json
from datetime import datetime
from typing import Optional
from concurrent.futures import ThreadPoolExecutor, as_completed

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.utils.config.config_loader import load_config_directory, load_yaml_file
from src.utils.logging.colored_logger import setup_colored_logging
from src.storage.database.sqlite_layer import UpdateDataLayer
from src.analyzers.update_analyzer import UpdateAnalyzer


class AnalyzeUpdatesScript:
    """æ›´æ–°è®°å½•åˆ†æè„šæœ¬"""
    
    def __init__(self, args):
        """
        åˆå§‹åŒ–è„šæœ¬
        
        Args:
            args: å‘½ä»¤è¡Œå‚æ•°
        """
        self.args = args
        
        # è®¾ç½®æ—¥å¿—
        log_level = logging.DEBUG if args.verbose else logging.INFO
        setup_colored_logging(level=log_level)
        self.logger = logging.getLogger('analyze_updates')
        
        # åŠ è½½é…ç½®
        self.config = self._load_config()
        
        # åˆå§‹åŒ–æ•°æ®åº“å±‚
        self.data_layer = UpdateDataLayer()
        
        # åˆå§‹åŒ–åˆ†æå™¨
        try:
            ai_config = self.config.get('ai_model', {})
            self.analyzer = UpdateAnalyzer(ai_config)
            self.logger.info("åˆ†æå™¨åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            self.logger.error(f"åˆ†æå™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            sys.exit(1)
    
    def _load_config(self):
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        try:
            # åŠ è½½ä¸»é…ç½®
            project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
            config_dir = os.path.join(project_root, 'config')
            main_config = load_config_directory(config_dir)
            
            # åŠ è½½ AI æ¨¡å‹é…ç½®
            ai_config_path = os.path.join(config_dir, 'ai_model.yaml')
            
            if os.path.exists(ai_config_path):
                ai_config_full = load_yaml_file(ai_config_path)
                # ä½¿ç”¨ 'default' èŠ‚ç‚¹è€Œä¸æ˜¯ 'ai_model'
                main_config['ai_model'] = ai_config_full.get('default', {})
            else:
                self.logger.error(f"AI æ¨¡å‹é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {ai_config_path}")
                sys.exit(1)
            
            return main_config
            
        except Exception as e:
            self.logger.error(f"åŠ è½½é…ç½®å¤±è´¥: {e}")
            sys.exit(1)
    
    def run(self):
        """æ‰§è¡Œåˆ†æä»»åŠ¡"""
        if self.args.update_id:
            # å•æ¡åˆ†ææ¨¡å¼
            self._analyze_single()
        elif self.args.batch:
            # å¤š ID æ‰¹é‡åˆ†ææ¨¡å¼
            self._analyze_by_ids()
        else:
            # æ‰¹é‡åˆ†ææ¨¡å¼ï¼ˆé»˜è®¤ï¼‰
            self._analyze_batch()
    
    def _analyze_single(self):
        """åˆ†æå•æ¡è®°å½•"""
        update_id = self.args.update_id
        
        self.logger.info(f"å¼€å§‹åˆ†æå•æ¡è®°å½•: {update_id}")
        
        # æŸ¥è¯¢è®°å½•
        update_data = self.data_layer.get_update_by_id(update_id)
        
        if not update_data:
            self.logger.error(f"æœªæ‰¾åˆ°è®°å½•: {update_id}")
            return
        
        # æ£€æŸ¥æ˜¯å¦å·²åˆ†æ
        if update_data.get('title_translated') and not self.args.force:
            self.logger.warning(f"è®°å½•å·²åˆ†æè¿‡ï¼Œè·³è¿‡ï¼ˆä½¿ç”¨ --force å¼ºåˆ¶é‡æ–°åˆ†æï¼‰")
            return
        
        # æ‰§è¡Œåˆ†æ
        result = self.analyzer.analyze(update_data)
        
        if result:
            # ä¿å­˜åˆ†æç»“æœåˆ°æ–‡ä»¶
            file_path = self._save_analysis_to_file(update_id, update_data, result)
            if file_path:
                self.logger.info(f"ğŸ“„ åˆ†æç»“æœå·²ä¿å­˜è‡³: {file_path}")
                # å›å†™æ–‡ä»¶è·¯å¾„åˆ°æ•°æ®åº“
                result['analysis_filepath'] = file_path
            
            # æ›´æ–°æ•°æ®åº“
            if not self.args.dry_run:
                success = self.data_layer.update_analysis_fields(update_id, result)
                if success:
                    self.logger.info(f"âœ… åˆ†ææˆåŠŸå¹¶å·²ä¿å­˜")
                else:
                    self.logger.error(f"âŒ æ•°æ®åº“æ›´æ–°å¤±è´¥")
            else:
                self.logger.info(f"âœ… åˆ†ææˆåŠŸï¼ˆé¢„è§ˆæ¨¡å¼ï¼Œæœªå†™å…¥æ•°æ®åº“ï¼‰")
                self.logger.info(f"åˆ†æç»“æœ:\n{self._format_result(result)}")
        else:
            self.logger.error(f"âŒ åˆ†æå¤±è´¥")
    
    def _save_analysis_to_file(self, update_id: str, update_data: dict, result: dict) -> Optional[str]:
        """
        ä¿å­˜åˆ†æç»“æœåˆ°æ–‡ä»¶
        
        Args:
            update_id: æ›´æ–°è®°å½• ID
            update_data: åŸå§‹æ›´æ–°æ•°æ®
            result: åˆ†æç»“æœ
            
        Returns:
            ä¿å­˜çš„æ–‡ä»¶è·¯å¾„ï¼Œå¤±è´¥è¿”å› None
        """
        try:
            # åˆ›å»ºè¾“å‡ºç›®å½•
            project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
            vendor = update_data.get('vendor', 'unknown')
            output_dir = os.path.join(project_root, 'data', 'analyzed', vendor)
            os.makedirs(output_dir, exist_ok=True)
            
            # æ„å»ºå®Œæ•´çš„åˆ†ææ•°æ®
            analysis_data = {
                'update_id': update_id,
                'vendor': vendor,
                'source_channel': update_data.get('source_channel', ''),
                'original_title': update_data.get('title', ''),
                'source_url': update_data.get('source_url', ''),
                'publish_date': update_data.get('publish_date', ''),
                'analyzed_at': datetime.now().isoformat(),
                'analysis': {
                    'title_translated': result.get('title_translated', ''),
                    'content_summary': result.get('content_summary', ''),
                    'update_type': result.get('update_type', ''),
                    'product_subcategory': result.get('product_subcategory', ''),
                    'tags': json.loads(result.get('tags', '[]')) if isinstance(result.get('tags'), str) else result.get('tags', [])
                }
            }
            
            # ç”Ÿæˆæ–‡ä»¶å
            filename = f"{update_id}.json"
            file_path = os.path.join(output_dir, filename)
            
            # å†™å…¥æ–‡ä»¶
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(analysis_data, f, ensure_ascii=False, indent=2)
            
            return file_path
            
        except Exception as e:
            self.logger.error(f"ä¿å­˜åˆ†æç»“æœåˆ°æ–‡ä»¶å¤±è´¥: {e}")
            return None
    
    def _analyze_by_ids(self):
        """æŒ‰æŒ‡å®š ID åˆ—è¡¨åˆ†æ"""
        # è§£æ ID åˆ—è¡¨ï¼ˆæ”¯æŒé€—å·åˆ†éš”ï¼‰
        id_list = [id.strip() for id in self.args.batch.split(',') if id.strip()]
        
        if not id_list:
            self.logger.error("æœªæä¾›æœ‰æ•ˆçš„ ID")
            return
        
        self.logger.info(f"ğŸ”„ å¼€å§‹åˆ†æ {len(id_list)} æ¡æŒ‡å®šè®°å½•...")
        
        # è·å–è®°å½•
        updates = []
        for update_id in id_list:
            update_data = self.data_layer.get_update_by_id(update_id)
            if update_data:
                # æ£€æŸ¥æ˜¯å¦å·²åˆ†æ
                if update_data.get('title_translated') and not self.args.force:
                    self.logger.warning(f"è·³è¿‡å·²åˆ†æ: {update_id}ï¼ˆä½¿ç”¨ --force å¼ºåˆ¶ï¼‰")
                    continue
                updates.append(update_data)
            else:
                self.logger.warning(f"æœªæ‰¾åˆ°è®°å½•: {update_id}")
        
        if not updates:
            self.logger.info("æ²¡æœ‰å¾…å¤„ç†çš„è®°å½•")
            return
        
        # ç»Ÿè®¡å˜é‡
        process_count = len(updates)
        success_count = 0
        fail_count = 0
        start_time = time.time()
        
        self.logger.info(f"ğŸ“Š å¾…å¤„ç†è®°å½•: {process_count} æ¡")
        
        # å¹¶å‘å¤„ç†
        batch_config = self.config.get('ai_model', {}).get('batch_processing', {})
        max_workers = batch_config.get('max_workers', 5)
        self.logger.info(f"âš¡ å¹¶å‘çº¿ç¨‹æ•°: {max_workers}")
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_to_update = {
                executor.submit(self._analyze_single_item, update_data): update_data
                for update_data in updates
            }
            
            for idx, future in enumerate(as_completed(future_to_update), 1):
                update_data = future_to_update[future]
                update_id = update_data.get('update_id', 'unknown')
                
                try:
                    success = future.result()
                    if success:
                        success_count += 1
                        self.logger.debug(f"âœ“ {idx}/{process_count} {update_id}")
                    else:
                        fail_count += 1
                        self.logger.warning(f"âœ— {idx}/{process_count} {update_id} - åˆ†æå¤±è´¥")
                except Exception as e:
                    fail_count += 1
                    self.logger.error(f"âœ— {idx}/{process_count} {update_id} - å¼‚å¸¸: {e}")
                
                self._print_progress(idx, process_count, success_count, fail_count, start_time)
        
        # æœ€ç»ˆè¿›åº¦å’Œç»Ÿè®¡
        self._print_progress(process_count, process_count, success_count, fail_count, start_time)
        total_time = time.time() - start_time
        self._print_summary(process_count, success_count, fail_count, total_time)

    def _analyze_batch(self):
        """æ‰¹é‡åˆ†æï¼ˆå¹¶å‘å¤„ç†ï¼‰"""
        # ç»Ÿè®¡å¾…å¤„ç†æ•°é‡
        total = self.data_layer.count_unanalyzed_updates(
            vendor=self.args.vendor,
            source_channel=self.args.source,
            include_analyzed=self.args.force
        )
        
        if total == 0:
            mode_desc = "è®°å½•" if self.args.force else "å¾…åˆ†æçš„è®°å½•"
            self.logger.info(f"æ²¡æœ‰{mode_desc}")
            return
        
        # ç¡®å®šå¤„ç†æ•°é‡
        limit = self.args.limit if self.args.limit else total
        process_count = min(limit, total)
        
        mode_desc = "å¼ºåˆ¶é‡æ–°åˆ†æ" if self.args.force else "æ‰¹é‡åˆ†æ"
        self.logger.info(f"ğŸ”„ å¼€å§‹{mode_desc}...")
        total_desc = "æ€»è®°å½•" if self.args.force else "æœªåˆ†æ"
        self.logger.info(f"ğŸ“Š å¾…å¤„ç†è®°å½•: {process_count} æ¡ï¼ˆå…± {total} æ¡{total_desc}ï¼‰")
        
        # è·å–å¾…å¤„ç†è®°å½•
        updates = self.data_layer.get_unanalyzed_updates(
            limit=limit,
            vendor=self.args.vendor,
            source_channel=self.args.source,
            include_analyzed=self.args.force
        )
        
        # ç»Ÿè®¡å˜é‡
        success_count = 0
        fail_count = 0
        start_time = time.time()
        
        # å¹¶å‘çº¿ç¨‹æ•°ï¼šä»é…ç½®è¯»å–
        batch_config = self.config.get('ai_model', {}).get('batch_processing', {})
        max_workers = batch_config.get('max_workers', 5)
        self.logger.info(f"âš¡ å¹¶å‘çº¿ç¨‹æ•°: {max_workers}")
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘å¤„ç†
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤ä»»åŠ¡
            future_to_update = {
                executor.submit(self._analyze_single_item, update_data): update_data
                for update_data in updates
            }
            
            # å¤„ç†å®Œæˆçš„ä»»åŠ¡
            for idx, future in enumerate(as_completed(future_to_update), 1):
                update_data = future_to_update[future]
                update_id = update_data.get('update_id', 'unknown')
                
                try:
                    # è·å–ç»“æœ
                    success = future.result()
                    
                    if success:
                        success_count += 1
                        self.logger.debug(f"âœ“ {idx}/{process_count} {update_id}")
                    else:
                        fail_count += 1
                        self.logger.warning(f"âœ— {idx}/{process_count} {update_id} - åˆ†æå¤±è´¥")
                        
                except Exception as e:
                    fail_count += 1
                    self.logger.error(f"âœ— {idx}/{process_count} {update_id} - å¼‚å¸¸: {e}")
                
                # æ˜¾ç¤ºè¿›åº¦
                self._print_progress(idx, process_count, success_count, fail_count, start_time)
        
        # æœ€ç»ˆè¿›åº¦
        self._print_progress(process_count, process_count, success_count, fail_count, start_time)
        
        # æ˜¾ç¤ºç»Ÿè®¡
        total_time = time.time() - start_time
        self._print_summary(process_count, success_count, fail_count, total_time)
    
    def _analyze_single_item(self, update_data: dict) -> bool:
        """
        åˆ†æå•æ¡è®°å½•ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰
        
        Args:
            update_data: æ›´æ–°æ•°æ®
            
        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        update_id = update_data.get('update_id', 'unknown')
        
        try:
            # æ‰§è¡Œåˆ†æ
            result = self.analyzer.analyze(update_data)
            
            if result:
                # ä¿å­˜åˆ†æç»“æœåˆ°æ–‡ä»¶
                file_path = self._save_analysis_to_file(update_id, update_data, result)
                if file_path:
                    # å›å†™æ–‡ä»¶è·¯å¾„åˆ° result
                    result['analysis_filepath'] = file_path
                
                # æ›´æ–°æ•°æ®åº“
                if not self.args.dry_run:
                    return self.data_layer.update_analysis_fields(update_id, result)
                else:
                    return True
            else:
                return False
                
        except Exception as e:
            self.logger.error(f"åˆ†æå¼‚å¸¸ {update_id}: {e}")
            return False
    
    def _print_progress(self, current, total, success, fail, start_time):
        """æ‰“å°è¿›åº¦æ¡"""
        percent = (current / total) * 100 if total > 0 else 0
        elapsed = time.time() - start_time
        
        # è®¡ç®—è¿›åº¦æ¡
        bar_length = 20
        filled = int(bar_length * current / total) if total > 0 else 0
        bar = 'â–ˆ' * filled + 'â–‘' * (bar_length - filled)
        
        # æ ¼å¼åŒ–æ—¶é—´
        elapsed_str = self._format_time(elapsed)
        
        # æ‰“å°ï¼ˆè¦†ç›–å½“å‰è¡Œï¼‰
        print(f"\r[{bar}] {current}/{total} ({percent:.1f}%) | "
              f"æˆåŠŸ: {success} | å¤±è´¥: {fail} | è€—æ—¶: {elapsed_str}", end='', flush=True)
    
    def _print_summary(self, total, success, fail, elapsed):
        """æ‰“å°ç»Ÿè®¡æ‘˜è¦"""
        print("\n")  # æ¢è¡Œ
        self.logger.info("âœ… åˆ†æå®Œæˆ!")
        self.logger.info(f"æ€»è®¡: {total} æ¡")
        self.logger.info(f"æˆåŠŸ: {success} æ¡ ({success/total*100:.1f}%)")
        self.logger.info(f"å¤±è´¥: {fail} æ¡ ({fail/total*100:.1f}%)")
        self.logger.info(f"æ€»è€—æ—¶: {self._format_time(elapsed)}")
    
    def _format_time(self, seconds):
        """æ ¼å¼åŒ–æ—¶é—´"""
        if seconds < 60:
            return f"{seconds:.0f}s"
        elif seconds < 3600:
            minutes = seconds / 60
            return f"{minutes:.0f}m{seconds%60:.0f}s"
        else:
            hours = seconds / 3600
            minutes = (seconds % 3600) / 60
            return f"{hours:.0f}h{minutes:.0f}m"
    
    def _format_result(self, result):
        """æ ¼å¼åŒ–åˆ†æç»“æœ"""
        lines = []
        for key, value in result.items():
            if key == 'tags':
                lines.append(f"  {key}: {value}")
            else:
                # æˆªæ–­è¿‡é•¿çš„å†…å®¹
                value_str = str(value)
                if len(value_str) > 100:
                    value_str = value_str[:100] + "..."
                lines.append(f"  {key}: {value_str}")
        return "\n".join(lines)


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description='æ›´æ–°è®°å½• AI åˆ†æå·¥å…·',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog='''
ç¤ºä¾‹:
  # æ‰¹é‡åˆ†æï¼ˆé»˜è®¤ï¼‰
  %(prog)s

  # åˆ†æå•æ¡è®°å½•
  %(prog)s --update-id abc123

  # æ‰¹é‡åˆ†æå‰ 100 æ¡
  %(prog)s --limit 100

  # ä»…åˆ†æ AWS è®°å½•
  %(prog)s --vendor aws

  # é¢„è§ˆæ¨¡å¼ï¼ˆä¸å†™å…¥æ•°æ®åº“ï¼‰
  %(prog)s --limit 10 --dry-run
        '''
    )
    
    # å•æ¡/å¤šæ¡åˆ†æé€‰é¡¹
    parser.add_argument(
        '--update-id',
        type=str,
        help='åˆ†ææŒ‡å®š ID çš„æ›´æ–°è®°å½•'
    )
    parser.add_argument(
        '--batch',
        type=str,
        help='æ‰¹é‡åˆ†æå¤šä¸ªæŒ‡å®š IDï¼ˆé€—å·åˆ†éš”ï¼Œå¦‚: id1,id2,id3ï¼‰'
    )
    parser.add_argument(
        '--limit',
        type=int,
        help='é™åˆ¶æ‰¹é‡å¤„ç†æ•°é‡'
    )
    parser.add_argument(
        '--vendor',
        type=str,
        choices=['aws', 'azure', 'gcp', 'huawei', 'tencentcloud', 'volcengine'],
        help='ä»…åˆ†ææŒ‡å®šå‚å•†çš„è®°å½•'
    )
    parser.add_argument(
        '--source',
        type=str,
        help='ä»…åˆ†ææŒ‡å®šæ•°æ®æºç±»å‹ï¼ˆå¦‚ blog, whatsnewï¼‰'
    )
    
    # é€šç”¨é€‰é¡¹
    parser.add_argument(
        '--dry-run',
        action='store_true',
        help='é¢„è§ˆæ¨¡å¼ï¼Œä¸å®é™…å†™å…¥æ•°æ®åº“'
    )
    parser.add_argument(
        '--force',
        action='store_true',
        help='å¼ºåˆ¶é‡æ–°åˆ†æå·²åˆ†æè¿‡çš„è®°å½•'
    )
    parser.add_argument(
        '--verbose',
        action='store_true',
        help='æ˜¾ç¤ºè¯¦ç»†æ—¥å¿—'
    )
    
    args = parser.parse_args()
    
    # æ‰§è¡Œè„šæœ¬
    script = AnalyzeUpdatesScript(args)
    script.run()


if __name__ == '__main__':
    main()
